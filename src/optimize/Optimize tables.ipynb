{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a81088f0-5b42-4d52-8d94-8886165d6604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "COMPUTE STORAGE METRICS is not available on our cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a32e3861-d891-483d-9268-a1f4ee483fb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Lake Health Audit & Maintenance Logic\n",
    "\n",
    "## 1. Pipeline Parameters\n",
    "The script accepts dynamic inputs from the orchestration pipeline (e.g., Azure Data Factory, Databricks Jobs) to control data retention safety.\n",
    "\n",
    "| Parameter Name | Default Value | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| `vacuum_retention_days` | **14** | The safety window for history retention. Files older than this are permanently removed during cleanup. |\n",
    "\n",
    "## 2. Standards & Thresholds\n",
    "We define specific metrics to categorize table health and trigger maintenance actions.\n",
    "\n",
    "| Metric | Threshold | Standard | Rationale |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Small Files** | `< 128 MB` | **File Efficiency** | Files smaller than 128 MB cause \"small file problem,\" slowing down read operations and increasing metadata overhead. |\n",
    "| **Zombie Bloat** | `> 10%` | **Storage Efficiency** | If inactive (deleted/overwritten) data exceeds 10% of total physical storage, it indicates wasted cost and storage. |\n",
    "| **Safety Limit** | `7 Days (Min)` | **Data Safety** | Delta Lake prevents `VACUUM` retention below 7 days by default to protect against data corruption during concurrent writes/reads. |\n",
    "\n",
    "## 3. Analysis Process\n",
    "The `TableHealthAuditor` performs a two-step analysis for every table in the scope:\n",
    "\n",
    "1.  **Logical Analysis (Metadata):**\n",
    "    * Runs `DESCRIBE DETAIL` to query the Delta Transaction Log.\n",
    "    * Retrieves the number of *active* files and *active* data size.\n",
    "    * Confirms the table is accessible and identifies partitioning columns.\n",
    "\n",
    "2.  **Physical Analysis (Storage Scan):**\n",
    "    * Recursively scans the underlying S3 bucket path using `dbutils.fs.ls`.\n",
    "    * Calculates the *total* physical size and *total* count of all objects in the directory.\n",
    "    * **Calculation:** `Inactive Data = Total Physical Size - Active Logical Size`.\n",
    "\n",
    "## 4. Maintenance Decisions & Conditions\n",
    "Based on the derived metrics, the script automatically executes the following commands:\n",
    "\n",
    "### Decision A: Run OPTIMIZE\n",
    "* **Condition:** Average File Size is **> 0 MB** AND **< 128 MB**.\n",
    "* **Action:** Executes `OPTIMIZE table_name`.\n",
    "* **Result:** Small files are compacted into larger files (target ~1GB) to improve read performance.\n",
    "\n",
    "### Decision B: Run VACUUM\n",
    "* **Condition:** Zombie/Bloat Percentage is **â‰¥ 10%**.\n",
    "* **Action:** Executes `VACUUM table_name RETAIN {retention_days} DAYS`.\n",
    "* **Result:** Physically deletes files that are no longer referenced by the Delta log and are older than the retention period.\n",
    "\n",
    "## 5. Limitations\n",
    "* **Recursive Listing Cost:** The physical S3 scan (`_get_physical_stats`) iterates through files individually. For tables with millions of files, this step can be slow and may hit API rate limits.\n",
    "* **Vacuum Safety:** The script respects the retention parameter strictly. If high storage waste exists but files are *newer* than the retention period (e.g., created yesterday), `VACUUM` will **not** delete them, ensuring time-travel capability is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed66698-9231-4688-bee9-3895002bafd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Pipeline Parameters\n",
    "# Initialize widget for retention period\n",
    "dbutils.widgets.text(\"vacuum_retention_days\", \"14\", \"Vacuum Retention (Days)\")\n",
    "\n",
    "# Validate and retrieve retention period\n",
    "try:\n",
    "    RETENTION_DAYS = int(dbutils.widgets.get(\"vacuum_retention_days\"))\n",
    "except ValueError:\n",
    "    raise ValueError(\"The 'vacuum_retention_days' parameter must be an integer.\")\n",
    "\n",
    "# Define the scope of the audit\n",
    "TABLE_SCOPE = [\"bronze_audit\", \"silver_audit\", \"quarantine_audit\"]\n",
    "\n",
    "class TableHealthAuditor:\n",
    "    # Maintenance Thresholds\n",
    "    SMALL_FILE_THRESHOLD_MB = 128.0\n",
    "    ZOMBIE_BLOAT_THRESHOLD_PCT = 10.0\n",
    "\n",
    "    def __init__(self, spark, dbutils, retention_days):\n",
    "        self.spark = spark\n",
    "        self.dbutils = dbutils\n",
    "        self.retention_days = retention_days\n",
    "        self.results = []\n",
    "\n",
    "    def _get_logical_stats(self, table_name):\n",
    "        \"\"\"Retrieves active metadata using DESCRIBE DETAIL.\"\"\"\n",
    "        try:\n",
    "            df = self.spark.sql(f\"DESCRIBE DETAIL {table_name}\")\n",
    "            stats = df.select(\"location\", \"numFiles\", \"sizeInBytes\", \"partitionColumns\").collect()[0]\n",
    "            return {\n",
    "                \"path\": stats[\"location\"],\n",
    "                \"active_files\": stats[\"numFiles\"],\n",
    "                \"active_bytes\": stats[\"sizeInBytes\"],\n",
    "                \"partitions\": stats[\"partitionColumns\"],\n",
    "                \"status\": \"ACCESSIBLE\"\n",
    "            }\n",
    "        except AnalysisException as e:\n",
    "            return {\"status\": \"MISSING\", \"error\": str(e)}\n",
    "\n",
    "    def _get_physical_stats(self, path):\n",
    "        \"\"\"Recursively scans S3 to find the actual storage footprint.\"\"\"\n",
    "        total_size = 0\n",
    "        total_files = 0\n",
    "        \n",
    "        # Iterative stack to avoid recursion depth limits\n",
    "        stack = [path]\n",
    "        \n",
    "        try:\n",
    "            while stack:\n",
    "                current_path = stack.pop()\n",
    "                items = self.dbutils.fs.ls(current_path)\n",
    "                \n",
    "                for item in items:\n",
    "                    if item.isDir:\n",
    "                        stack.append(item.path)\n",
    "                    else:\n",
    "                        total_size += item.size\n",
    "                        total_files += 1\n",
    "        except Exception as e:\n",
    "            # Handle permissions or path not found\n",
    "            return 0, 0, str(e)\n",
    "            \n",
    "        return total_size, total_files, None\n",
    "\n",
    "    def _perform_maintenance(self, table_name, avg_file_size_mb, bloat_pct):\n",
    "        \"\"\"Executes OPTIMIZE or VACUUM based on calculated metrics.\"\"\"\n",
    "        print(f\"[INFO] Evaluating maintenance for {table_name}...\")\n",
    "\n",
    "        # Condition 1: OPTIMIZE (Small Files)\n",
    "        if 0 < avg_file_size_mb < self.SMALL_FILE_THRESHOLD_MB:\n",
    "            print(f\"[WARNING] Average file size ({avg_file_size_mb:.2f} MB) is below threshold. Executing OPTIMIZE...\")\n",
    "            try:\n",
    "                self.spark.sql(f\"OPTIMIZE {table_name}\")\n",
    "                print(f\"[INFO] OPTIMIZE command completed for {table_name}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to OPTIMIZE {table_name}: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"[INFO] File size is healthy. OPTIMIZE skipped.\")\n",
    "\n",
    "        # Condition 2: VACUUM (Storage Bloat)\n",
    "        if bloat_pct >= self.ZOMBIE_BLOAT_THRESHOLD_PCT:\n",
    "            print(f\"[WARNING] Storage waste ({bloat_pct:.2f}%) exceeds limit. Executing VACUUM with {self.retention_days} days retention...\")\n",
    "            try:\n",
    "                self.spark.sql(f\"VACUUM {table_name} RETAIN {self.retention_days} DAYS\")\n",
    "                print(f\"[INFO] VACUUM command completed for {table_name}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to VACUUM {table_name}: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"[INFO] Storage waste is within limits. VACUUM skipped.\")\n",
    "\n",
    "    def analyze_table(self, table_name):\n",
    "        print(f\"Analyzing {table_name}...\")\n",
    "        \n",
    "        # 1. Logical Stats (Delta Metadata)\n",
    "        logical = self._get_logical_stats(table_name)\n",
    "        if logical[\"status\"] != \"ACCESSIBLE\":\n",
    "            print(f\"[ERROR] Could not analyze {table_name}: {logical.get('error')}\")\n",
    "            return\n",
    "\n",
    "        # 2. Physical Stats (S3 Scan)\n",
    "        phy_bytes, phy_files, phy_err = self._get_physical_stats(logical[\"path\"])\n",
    "        \n",
    "        # 3. Calculate Derived Metrics\n",
    "        active_bytes = logical[\"active_bytes\"]\n",
    "        inactive_bytes = phy_bytes - active_bytes\n",
    "        bloat_pct = (inactive_bytes / phy_bytes * 100) if phy_bytes > 0 else 0\n",
    "        \n",
    "        avg_file_size_mb = (active_bytes / logical[\"active_files\"] / (1024*1024)) if logical[\"active_files\"] > 0 else 0\n",
    "        \n",
    "        # 4. Trigger Maintenance Actions\n",
    "        self._perform_maintenance(table_name, avg_file_size_mb, bloat_pct)\n",
    "\n",
    "        # 5. Determine Reporting Status (Post-Maintenance check reference)\n",
    "        if avg_file_size_mb < 100:\n",
    "            file_status = \"WARNING: Small Files\"\n",
    "        elif avg_file_size_mb > 2000:\n",
    "            file_status = \"WARNING: Oversized Files\"\n",
    "        else:\n",
    "            file_status = \"OK\"\n",
    "\n",
    "        if bloat_pct > 30:\n",
    "            vacuum_status = \"WARNING: High Storage Waste\"\n",
    "        else:\n",
    "            vacuum_status = \"OK\"\n",
    "\n",
    "        # 6. Compile Report\n",
    "        report = {\n",
    "            \"Table Name\": table_name,\n",
    "            \"Partitioning\": str(logical[\"partitions\"]),\n",
    "            \"Active Size (MB)\": round(active_bytes / (1024*1024), 2),\n",
    "            \"Total S3 Size (MB)\": round(phy_bytes / (1024*1024), 2),\n",
    "            \"Inactive Data (MB)\": round(inactive_bytes / (1024*1024), 2),\n",
    "            \"Waste %\": round(bloat_pct, 2),\n",
    "            \"Avg File Size (MB)\": round(avg_file_size_mb, 2),\n",
    "            \"File Health\": file_status,\n",
    "            \"Storage Health\": vacuum_status\n",
    "        }\n",
    "        self.results.append(report)\n",
    "\n",
    "    def print_formal_report(self):\n",
    "        if not self.results:\n",
    "            print(\"No results to display.\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"{'DATA LAKE HEALTH AUDIT REPORT':^80}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "        for row in self.results:\n",
    "            print(f\"TABLE: {row['Table Name']}\")\n",
    "            print(\"-\" * 40)\n",
    "            print(f\"Configuration:\")\n",
    "            print(f\"  Partition Strategy   : {row['Partitioning']}\")\n",
    "            \n",
    "            print(f\"\\nStorage Metrics:\")\n",
    "            print(f\"  Active Data Volume   : {row['Active Size (MB)']} MB\")\n",
    "            print(f\"  Total Physical usage : {row['Total S3 Size (MB)']} MB\")\n",
    "            print(f\"  Inactive / History   : {row['Inactive Data (MB)']} MB ({row['Waste %']}%)\")\n",
    "            print(f\"  Status               : {row['Storage Health']}\")\n",
    "\n",
    "            print(f\"\\nFile Efficiency:\")\n",
    "            print(f\"  Average File Size    : {row['Avg File Size (MB)']} MB\")\n",
    "            print(f\"  Status               : {row['File Health']}\")\n",
    "            print(\"\\n\" + \".\"*80 + \"\\n\")\n",
    "\n",
    "# --- EXECUTION ---\n",
    "# Pass retention_days to the Auditor\n",
    "auditor = TableHealthAuditor(spark, dbutils, RETENTION_DAYS)\n",
    "\n",
    "# Iterate through the defined tables\n",
    "for table in TABLE_SCOPE:\n",
    "    auditor.analyze_table(table)\n",
    "\n",
    "# Display final formal text report\n",
    "auditor.print_formal_report()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4597829049632243,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Optimize tables",
   "widgets": {
    "vacuum_retention_days": {
     "currentValue": "14",
     "nuid": "40d290cd-a143-4f11-9906-9e8fd38d58b2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "14",
      "label": "Vacuum Retention (Days)",
      "name": "vacuum_retention_days",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "14",
      "label": "Vacuum Retention (Days)",
      "name": "vacuum_retention_days",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
