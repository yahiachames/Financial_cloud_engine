{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea2026ae-5e01-4ea9-b206-4008f2f9fbed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3a644ca-ac8e-40f8-8e89-52e7edc14d29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- CONFIG ---\n",
    "# 1. Inputs\n",
    "speed_path = \"/Volumes/workspace/default/storage/gold/ticker_data_v7\" # Streaming Delta\n",
    "batch_path = \"gold_valid_audit\" # Static Delta (S3)\n",
    "\n",
    "# 2. Output\n",
    "serving_path = \"/Volumes/workspace/default/storage/serving/valuation_dashboard_v4\"\n",
    "checkpoint_path = \"/Volumes/workspace/default/storage/checkpoints/job_serving_dashboard_v4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a20e619b-9059-4f36-8c67-e1585d410e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Credentials (Required for S3 Access)\n",
    "ACCESS_KEY = dbutils.secrets.get(scope=\"ticker\", key=\"access_key\")\n",
    "SECRET_KEY = dbutils.secrets.get(scope=\"ticker\", key=\"secret_key\")\n",
    "SESSION_TOKEN = dbutils.secrets.get(scope=\"ticker\", key=\"session_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d4245cf-1bac-4593-a369-d436a177ce7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. READ SPEED LAYER (STREAMING) ---\n",
    "# This is already in Volumes, so no Keys needed here\n",
    "df_speed = spark.readStream.format(\"delta\").load(speed_path).withColumnRenamed(\"s\", \"symbol\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd139078-399b-451b-b3f4-f52848597d6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 2. READ BATCH LAYER (STATIC S3) ---\n",
    "# We inject keys to read the S3 data\n",
    "df_batch_raw = (spark.read\n",
    "    .format(\"delta\")\n",
    "    .option(\"fs.s3a.access.key\", ACCESS_KEY)\n",
    "    .option(\"fs.s3a.secret.key\", SECRET_KEY)\n",
    "    .option(\"fs.s3a.session.token\", SESSION_TOKEN)\n",
    "    .option(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "    .table(batch_path)\n",
    "    \n",
    ")\n",
    "df_batch_raw.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "257cf4a3-ee36-463f-a532-f52df3ca1e5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 3. PREPARE BATCH (FORWARD FILL) ---\n",
    "# We only want the LATEST financial report for each symbol\n",
    "window_spec = Window.partitionBy(\"symbol\").orderBy(F.col(\"date\").desc())\n",
    "\n",
    "df_batch_latest = df_batch_raw.withColumn(\"rank\", F.row_number().over(window_spec)) \\\n",
    "                              .filter(\"rank = 1\") \\\n",
    "                              .drop(\"rank\") \\\n",
    "                              .withColumnRenamed(\"date\", \"report_date\")\n",
    "\n",
    "# --- 4. CALCULATE BATCH RATIOS (PRE-CALC) ---\n",
    "# Calculate Cost of Debt & Tax Rate once (Static)\n",
    "# Handle Div/0 safely using nullif\n",
    "df_batch_prepared = df_batch_latest.withColumn(\n",
    "    \"cost_of_debt\", \n",
    "    F.col(\"interest_expense\") / F.nullif(F.col(\"total_debt\"), F.lit(0))\n",
    ").withColumn(\n",
    "    \"effective_tax_rate\",\n",
    "    F.col(\"tax_expense\") / F.nullif((F.col(\"nopat\") + F.col(\"tax_expense\")), F.lit(0)) \n",
    "    # Approx Pre-Tax Income = NOPAT + Tax\n",
    ").select(\n",
    "    \"symbol\", \"report_date\", \"shares_outstanding\", \n",
    "    \"net_debt\", \"total_debt\", \"cost_of_debt\", \"effective_tax_rate\",\n",
    "    \"calculated_fcf\", \"nopat\" # Keep FCF for dashboard\n",
    ")\n",
    "\n",
    "# --- 5. STREAM-STATIC JOIN ---\n",
    "# Left Join: Speed is the master. If Batch is missing, we still show Price/Beta.\n",
    "df_serving = df_speed.join(F.broadcast(df_batch_prepared), on=\"symbol\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0db341f8-5466-4e5f-821c-54c599709eb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 6. VALUATION LOGIC (THE WACC ENGINE) ---\n",
    "\n",
    "# A. Market Cap = Price * Shares\n",
    "market_cap = F.col(\"close_price\") * F.col(\"shares_outstanding\")\n",
    "\n",
    "# B. Enterprise Value (V) = Equity + Debt\n",
    "# Note: We use Total Debt for WACC weighting, Net Debt for EV Valuation\n",
    "total_capital = market_cap + F.col(\"total_debt\")\n",
    "enterprise_value = market_cap + F.col(\"net_debt\")\n",
    "\n",
    "# C. Weights\n",
    "weight_equity = market_cap / F.nullif(total_capital, F.lit(0))\n",
    "weight_debt = F.col(\"total_debt\") / F.nullif(total_capital, F.lit(0))\n",
    "\n",
    "# D. WACC Formula\n",
    "# WACC = (We * Ke) + (Wd * Kd * (1 - T))\n",
    "wacc_calc = (\n",
    "    (weight_equity * F.col(\"cost_of_equity\")) + \n",
    "    (weight_debt * F.col(\"cost_of_debt\") * (1 - F.col(\"effective_tax_rate\")))\n",
    ")\n",
    "\n",
    "# E. PE Ratio\n",
    "# EPS Proxy = NOPAT / Shares (Simplified)\n",
    "eps_proxy = F.col(\"nopat\") / F.nullif(F.col(\"shares_outstanding\"), F.lit(0))\n",
    "pe_ratio = F.col(\"close_price\") / F.nullif(eps_proxy, F.lit(0))\n",
    "\n",
    "# --- 7. APPLY TRANSFORMATION ---\n",
    "df_final = df_serving.withColumn(\"market_cap\", market_cap) \\\n",
    "                     .withColumn(\"enterprise_value\", enterprise_value) \\\n",
    "                     .withColumn(\"wacc\", wacc_calc) \\\n",
    "                     .withColumn(\"pe_ratio_implied\", pe_ratio) \\\n",
    "                     .withColumn(\"valuation_timestamp\", F.current_timestamp()) \\\n",
    "                     .withColumn(\"valuation_date\", F.to_date(F.current_timestamp())) \n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af89ed8e-a625-4218-92f2-8ffee76e1e06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 8. SELECT FINAL DASHBOARD COLUMNS ---\n",
    "output_schema = [\n",
    "    # Identity\n",
    "    \"symbol\", \"valuation_timestamp\", \"report_date\",  \"valuation_date\",\n",
    "    # Speed Metrics\n",
    "    \"close_price\", \"beta\", \"volatility\", \"momentum\", \"cost_of_equity\",\n",
    "    # Batch Metrics\n",
    "    \"calculated_fcf\", \"cost_of_debt\", \"effective_tax_rate\",\n",
    "    # Synthesis\n",
    "    \"market_cap\", \"enterprise_value\", \"wacc\", \"pe_ratio_implied\"\n",
    "]\n",
    "\n",
    "df_dashboard = df_final.select(*output_schema)\n",
    "\n",
    "# --- 9. WRITE STREAM (TO S3) ---\n",
    "print(f\"Starting Serving Stream... Writing to {serving_path}\")\n",
    "\n",
    "df_dashboard.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .option(\"fs.s3a.access.key\", ACCESS_KEY) \\\n",
    "    .option(\"fs.s3a.secret.key\", SECRET_KEY) \\\n",
    "    .option(\"fs.s3a.session.token\", SESSION_TOKEN) \\\n",
    "    .option(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .partitionBy(\"valuation_date\") \\\n",
    "    .start(serving_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "serving layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
