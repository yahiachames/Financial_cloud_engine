{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "ea2026ae-5e01-4ea9-b206-4008f2f9fbed",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "# Serving Layer: The Valuation Engine\n",
                "\n",
                "## architecture: Hybrid Stream-Batch Join\n",
                "This pipeline implements a **Lambda Architecture** pattern (Speed + Batch) to derive real-time financial valuations.\n",
                "\n",
                "1.  **Speed Layer (Stream):** Contains real-time market data (Price, Beta, Volatility) updated frequently.\n",
                "2.  **Batch Layer (Static):** Contains quarterly financial reports (Debt, Tax Expense, NOPAT) stored in S3.\n",
                "3.  **Serving Layer (Join):** We merge these two timelines. Since financial reports are sparse (quarterly) and prices are dense (daily/minutely), we use a **Forward-Fill** strategy to attribute the latest available financial report to every real-time price point.\n",
                "\n",
                "## Key Metrics\n",
                "*   **WACC (Weighted Average Cost of Capital):** Calculated dynamically using real-time Market Cap weights.\n",
                "*   **Enterprise Value:** Market Cap + Net Debt.\n",
                "*   **Implied PE:** Price / (NOPAT/Shares)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "d3a644ca-ac8e-40f8-8e89-52e7edc14d29",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "import pyspark.sql.functions as F\n",
                "from pyspark.sql.window import Window\n",
                "\n",
                "# --- CONFIGURATION ---\n",
                "\n",
                "# 1. INPUT SOURCES\n",
                "speed_path = \"/Volumes/workspace/default/storage/gold/ticker_data_v10\" # Streaming Delta (Volumes)\n",
                "batch_path = \"gold_valid_audit\" # Static Delta (Managed Table / S3)\n",
                "\n",
                "# 2. OUTPUT DESTINATIONS\n",
                "serving_path = \"/Volumes/workspace/default/storage/serving/valuation_dashboard_v5\"\n",
                "checkpoint_path = \"/Volumes/workspace/default/storage/checkpoints/job_serving_dashboard_v5\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "7d4245cf-1bac-4593-a369-d436a177ce7e",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "# --- 1. READ SPEED LAYER (STREAMING) ---\n",
                "# Read the high-velocity market data from the Volume.\n",
                "# Note: In a production stream, we would use spark.readStream\n",
                "df_speed = spark.read.format(\"delta\").load(speed_path).withColumnRenamed(\"s\", \"symbol\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "bd139078-399b-451b-b3f4-f52848597d6a",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "# --- 2. READ BATCH LAYER (STATIC S3) ---\n",
                "# Access the registered managed table pointing to S3 data.\n",
                "df_batch_raw = (\n",
                "    spark.read\n",
                "    .format(\"delta\")\n",
                "    .table(batch_path)\n",
                ")\n",
                "df_batch_raw.show(10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "257cf4a3-ee36-463f-a532-f52df3ca1e5f",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "# --- 3. PREPARE BATCH (FORWARD FILL LOGIC) ---\n",
                "# We select ONLY the single latest financial report for each symbol.\n",
                "# This assumes the latest report is valid for all subsequent market data until a new report arrives.\n",
                "window_spec = Window.partitionBy(\"symbol\").orderBy(F.col(\"date\").desc())\n",
                "\n",
                "df_batch_latest = df_batch_raw.withColumn(\"rank\", F.row_number().over(window_spec)) \\\n",
                "                              .filter(\"rank = 1\") \\\n",
                "                              .drop(\"rank\") \\\n",
                "                              .withColumnRenamed(\"date\", \"report_date\")\n",
                "\n",
                "# --- 4. CALCULATE STATIC FINANCIAL RATIOS ---\n",
                "# These metrics are constant for the quarter (Debt, Tax Rate).\n",
                "# We use nullif(..., 0) to prevent DivisionByZero errors during calculation.\n",
                "df_batch_prepared = df_batch_latest.withColumn(\n",
                "    \"cost_of_debt\", \n",
                "    F.col(\"interest_expense\") / F.nullif(F.col(\"total_debt\"), F.lit(0))\n",
                ").withColumn(\n",
                "    \"effective_tax_rate\",\n",
                "    F.col(\"tax_expense\") / F.nullif((F.col(\"nopat\") + F.col(\"tax_expense\")), F.lit(0)) \n",
                "    # Formula: Approx Pre-Tax Income = NOPAT + Tax\n",
                ").select(\n",
                "    \"symbol\", \"report_date\", \"shares_outstanding\", \n",
                "    \"net_debt\", \"total_debt\", \"cost_of_debt\", \"effective_tax_rate\",\n",
                "    \"calculated_fcf\", \"nopat\" # Retained for Dashboard display\n",
                ")\n",
                "\n",
                "# --- 5. STREAM-STATIC JOIN ---\n",
                "# Strategy: Broadcase Left Join\n",
                "# - Left side (Speed) is large and distributed.\n",
                "# - Right side (Batch) is small (1 row per symbol).\n",
                "# - Result: Even if a symbol has no financial report (Batch), we still show its price (Speed).\n",
                "df_serving = df_speed.join(F.broadcast(df_batch_prepared), on=\"symbol\", how=\"left\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "0db341f8-5466-4e5f-821c-54c599709eb4",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "# --- 6. VALUATION LOGIC (THE WACC ENGINE) ---\n",
                "\n",
                "# A. Market Cap = Price * Shares\n",
                "market_cap = F.col(\"close_price\") * F.col(\"shares_outstanding\")\n",
                "\n",
                "# B. Enterprise Value (V) = Equity + Debt\n",
                "# Note: Total Debt is used for WACC weighting; Net Debt is used for EV calculation.\n",
                "total_capital = market_cap + F.col(\"total_debt\")\n",
                "enterprise_value = market_cap + F.col(\"net_debt\")\n",
                "\n",
                "# C. Weights (Equity vs Debt)\n",
                "weight_equity = market_cap / F.nullif(total_capital, F.lit(0))\n",
                "weight_debt = F.col(\"total_debt\") / F.nullif(total_capital, F.lit(0))\n",
                "\n",
                "# D. WACC Formula\n",
                "# WACC = (We * Ke) + (Wd * Kd * (1 - T))\n",
                "wacc_calc = (\n",
                "    (weight_equity * F.col(\"cost_of_equity\")) + \n",
                "    (weight_debt * F.col(\"cost_of_debt\") * (1 - F.col(\"effective_tax_rate\")))\n",
                ")\n",
                "\n",
                "# E. PE Ratio (Implied)\n",
                "# EPS Proxy = NOPAT / Shares (Simplified)\n",
                "eps_proxy = F.col(\"nopat\") / F.nullif(F.col(\"shares_outstanding\"), F.lit(0))\n",
                "pe_ratio = F.col(\"close_price\") / F.nullif(eps_proxy, F.lit(0))\n",
                "\n",
                "# --- 7. APPLY TRANSFORMATION ---\n",
                "df_final = df_serving.withColumn(\"market_cap\", market_cap) \\\n",
                "                     .withColumn(\"enterprise_value\", enterprise_value) \\\n",
                "                     .withColumn(\"wacc\", wacc_calc) \\\n",
                "                     .withColumn(\"pe_ratio_implied\", pe_ratio) \\\n",
                "                     .withColumn(\"valuation_timestamp\", F.current_timestamp()) \\\n",
                "                     .withColumn(\"valuation_date\", F.to_date(F.current_timestamp())) \n",
                "                     "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "af89ed8e-a625-4218-92f2-8ffee76e1e06",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "# --- 8. SELECT FINAL DASHBOARD COLUMNS ---\n",
                "output_schema = [\n",
                "    # Identity\n",
                "    \"symbol\", \"valuation_timestamp\", \"report_date\",  \"valuation_date\",\n",
                "    # Speed Metrics\n",
                "    \"close_price\", \"beta\", \"volatility\", \"momentum\", \"cost_of_equity\",\n",
                "    # Batch Metrics\n",
                "    \"calculated_fcf\", \"cost_of_debt\", \"effective_tax_rate\",\n",
                "    # Synthesis (Valuation)\n",
                "    \"market_cap\", \"enterprise_value\", \"wacc\", \"pe_ratio_implied\"\n",
                "]\n",
                "\n",
                "df_dashboard = df_final.select(*output_schema)\n",
                "\n",
                "# --- 9. WRITE TO SERVING LAYER ---\n",
                "print(f\"Starting Serving Layer Write to: {serving_path}\")\n",
                "\n",
                "# For batch backfill/initial load:\n",
                "df_dashboard.write \\\n",
                "    .format(\"delta\") \\\n",
                "    .mode(\"overwrite\") \\\n",
                "    .option(\"overwriteSchema\", \"true\") \\\n",
                "    .save(serving_path)\n",
                "    \n",
                "print(\"Success: Serving Layer updated.\")"
            ]
        }
    ],
    "metadata": {
        "application/vnd.databricks.v1+notebook": {
            "computePreferences": null,
            "dashboards": [],
            "environmentMetadata": {
                "base_environment": "",
                "environment_version": "4"
            },
            "inputWidgetPreferences": null,
            "language": "python",
            "notebookMetadata": {
                "mostRecentlyExecutedCommandWithImplicitDF": {
                    "commandId": 6200691363269828,
                    "dataframes": [
                        "_sqldf"
                    ]
                },
                "pythonIndentUnit": 4
            },
            "notebookName": "serving_layer",
            "widgets": {}
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}