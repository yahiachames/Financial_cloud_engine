{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5b67c8b-f777-49c1-b130-187bf7286189",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import ArrayType , StringType , StructField , StructType , LongType , DoubleType , IntegerType , BooleanType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea854483-97d8-4eb3-b44c-372bccea5255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "source_path = \"/Volumes/workspace/default/storage/silver/ticker_data_v5\"\n",
    "destination_path = \"/Volumes/workspace/default/storage/gold/ticker_data_v7\"\n",
    "# --- 2. THE FIX: PATH MUST INCLUDE THE VOLUME NAME ---\n",
    "# Path format: /Volumes/<catalog>/<schema>/<VOLUME_NAME>/<folder>\n",
    "# We added 'storage' because that is the volume we just created in SQL.\n",
    "checkpoint_path = \"/Volumes/workspace/default/storage/checkpoints/job_gold_ticker_checkpoint_v10\"\n",
    "\n",
    "print(f\"Streaming Strategy:\")\n",
    "print(f\"Checkpoint: {checkpoint_path}\")\n",
    "print(f\"Data (Volume): {source_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea1dd71a-589d-4242-ae09-a5b952321c1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Constants for CAPM Model (Can be replaced by dynamic macro data later)\n",
    "RISK_FREE_RATE = 0.0425  # 4.25% (10Y Treasury)\n",
    "MARKET_PREMIUM = 0.0500  # 5.00% (Standard assumption)\n",
    "# providing a starting version\n",
    "df = spark.readStream.format(\"delta\") \\\n",
    "  .option(\"startingVersion\", \"0\") \\\n",
    "  .load(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "203a26d8-b168-4ae4-b15a-c4ab5f0afad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. SEPARATE THE STREAMS ---\n",
    "# We split the logic. One stream calculates safe stats (Price/Vol), the other tries to calculate Beta.\n",
    "\n",
    "# A. TICKER STATS (Safe Stream)\n",
    "# This guarantees you ALWAYS get Close Price and Volatility, even if SPY is missing.\n",
    "df_ticker_stats = (df_tickers\n",
    "    .withWatermark(\"event_time\", \"1 hour\") # Handle late data\n",
    "    .groupBy(\n",
    "        F.col(\"s\"),\n",
    "        F.window(\"event_time\", \"2 hours\", \"5 minutes\")\n",
    "    )\n",
    "    .agg(\n",
    "        # Standard Deviation of PRICE (Native Volatility)\n",
    "        # This fixes your \"0 std\" issue because it runs on raw data before any join\n",
    "        (F.stddev_samp(\"currentPrice\") / F.avg(\"currentPrice\")).alias(\"volatility\"),\n",
    "        \n",
    "        # Momentum\n",
    "        F.avg(\"changePercent\").alias(\"momentum\"),\n",
    "        \n",
    "        # Close Price\n",
    "        F.last(\"currentPrice\").alias(\"close_price\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# B. BETA CALCULATION (Risky Stream)\n",
    "# This tries to join with SPY. If it fails, we handle it later.\n",
    "\n",
    "# Join Condition\n",
    "join_condition = (\n",
    "    F.col(\"m_join_key\") == F.col(\"join_key\")\n",
    ") & (\n",
    "    F.col(\"event_time\") >= F.col(\"m_event_time\") - F.expr(\"INTERVAL 1 HOUR\")\n",
    ") & (\n",
    "    F.col(\"event_time\") <= F.col(\"m_event_time\") + F.expr(\"INTERVAL 1 HOUR\")\n",
    ")\n",
    "\n",
    "# Use LEFT JOIN so we don't lose the Ticker windows even if SPY is missing\n",
    "df_beta_calc = df_tickers.join(df_market, join_condition, \"left_outer\")\n",
    "\n",
    "df_beta_agg = (df_beta_calc\n",
    "    .withWatermark(\"event_time\", \"1 hour\")\n",
    "    .groupBy(\n",
    "        F.col(\"s\"),\n",
    "        F.window(\"event_time\", \"2 hours\", \"5 minutes\")\n",
    "    )\n",
    "    .agg(\n",
    "        # Safe Beta Logic:\n",
    "        # If Variance is 0 or Null (due to missing SPY), return NULL\n",
    "        (F.covar_samp(\"changePercent\", \"m_return\") / F.nullif(F.var_samp(\"m_return\"), F.lit(0))).alias(\"calc_beta\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- 2. MERGE & FALLBACK (The Fix) ---\n",
    "\n",
    "# Join the Safe Stats with the Calculated Beta\n",
    "df_final = df_ticker_stats.join(\n",
    "    df_beta_agg,\n",
    "    on=[\"s\", \"window\"],\n",
    "    how=\"left_outer\" \n",
    ")\n",
    "\n",
    "# --- 3. APPLY DEFAULTS ---\n",
    "df_final = df_final.withColumn(\n",
    "    \"beta\",\n",
    "    # LOGIC: If Beta was calculated, use it. \n",
    "    # If it's NULL (because Join failed or SPY was missing), Default to 1.0 (Market Average).\n",
    "    F.coalesce(F.col(\"calc_beta\"), F.lit(1.0)) \n",
    ").withColumn(\n",
    "    \"cost_of_equity\",\n",
    "    # CAPM Model: Rf + Beta * (Rm - Rf)\n",
    "    F.lit(RISK_FREE_RATE) + (F.col(\"beta\") * F.lit(MARKET_PREMIUM))\n",
    ")\n",
    "\n",
    "# --- 4. DEBUG/OUTPUT ---\n",
    "# Select final columns for WACC\n",
    "df_result = df_final.select(\n",
    "    \"window\", \"s\", \"close_price\", \"volatility\", \"momentum\", \"beta\", \"cost_of_equity\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "723e4776-e832-4fb6-bd72-d1060634b1ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 3. WRITE STREAM ---\n",
    "df_result.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start(destination_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7404889362849864,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "gold_ticker_agg.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
