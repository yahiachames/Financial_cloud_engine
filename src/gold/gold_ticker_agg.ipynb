{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5b67c8b-f777-49c1-b130-187bf7286189",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import ArrayType , StringType , StructField , StructType , LongType , DoubleType , IntegerType , BooleanType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea854483-97d8-4eb3-b44c-372bccea5255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "source_path = \"/Volumes/workspace/default/storage/silver/ticker_data_v5\"\n",
    "destination_path = \"/Volumes/workspace/default/storage/gold/ticker_data_v10\"\n",
    "# --- 2. THE FIX: PATH MUST INCLUDE THE VOLUME NAME ---\n",
    "# Path format: /Volumes/<catalog>/<schema>/<VOLUME_NAME>/<folder>\n",
    "# We added 'storage' because that is the volume we just created in SQL.\n",
    "checkpoint_path = \"/Volumes/workspace/default/storage/checkpoints/job_gold_ticker_checkpoint_v17\"\n",
    "\n",
    "print(f\"Streaming Strategy:\")\n",
    "print(f\"Checkpoint: {checkpoint_path}\")\n",
    "print(f\"Data (Volume): {source_path}\")\n",
    "\n",
    "\n",
    "# --- 1. READ SOURCE ---\n",
    "df = spark.readStream.format(\"delta\") \\\n",
    "  .option(\"startingVersion\", \"0\") \\\n",
    "  .load(source_path) \\\n",
    "        .withColumn(\"dummy_join_key\", F.lit(1)) \\\n",
    "  .withWatermark(\"event_time\", \"40 seconds\") \n",
    "\n",
    "\n",
    "# backfill\n",
    "\n",
    "# df = spark.read.format(\"delta\") \\\n",
    "#   .option(\"startingVersion\", \"0\") \\\n",
    "#   .load(source_path) \\\n",
    "#         .withColumn(\"dummy_join_key\", F.lit(1)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea1dd71a-589d-4242-ae09-a5b952321c1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Constants for CAPM Model (Can be replaced by dynamic macro data later)\n",
    "RISK_FREE_RATE = 0.0425  # 4.25% (10Y Treasury)\n",
    "MARKET_PREMIUM = 0.0500  # 5.00% (Standard assumption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "203a26d8-b168-4ae4-b15a-c4ab5f0afad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "WINDOW_DURATION = \"3 hours\"\n",
    "SLIDE_DURATION = \"15 minutes\"\n",
    "JOIN_LOOKBACK = \"90 minutes\" # Half of the window for the recursive join\n",
    "\n",
    "# --- 2. BRANCH A: TICKER STATS (Volatility/Price) ---\n",
    "# Calculated on RAW data to ensure Standard Deviation is accurate (not biased by join duplication)\n",
    "df_ticker_stats = (df\n",
    "    .groupBy(\n",
    "        F.col(\"s\"),\n",
    "        F.window(\"event_time\", WINDOW_DURATION, SLIDE_DURATION)\n",
    "    )\n",
    "    .agg(\n",
    "        (F.stddev_samp(\"currentPrice\") / F.avg(\"currentPrice\")).alias(\"volatility\"),\n",
    "        F.avg(\"changePercent\").alias(\"momentum\"),\n",
    "        F.last(\"currentPrice\").alias(\"close_price\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- 3. BRANCH B: BETA CALCULATION (Recursive Join) ---\n",
    "\n",
    "# Prepare Market Stream (Derived from the same source 'df')\n",
    "df_market = df.filter(F.col(\"s\") == \"SPY\") \\\n",
    "    .select(\n",
    "        F.col(\"event_time\").alias(\"m_event_time\"),\n",
    "        F.col(\"changePercent\").alias(\"m_return\"),\n",
    "        F.col(\"s\").alias(\"m_join_key\"),\n",
    "        F.col(\"dummy_join_key\").alias(\"m_dummy_join_key\")\n",
    "    )\n",
    "\n",
    "# The \"Recursive\" Join Condition\n",
    "# Matches Ticker events with Market events within the valid window range\n",
    "join_condition = (F.col(\"dummy_join_key\") == F.col(\"m_dummy_join_key\")) & \\\n",
    "(\n",
    "    F.col(\"event_time\") >= F.col(\"m_event_time\") - F.expr(f\"INTERVAL {JOIN_LOOKBACK}\")\n",
    ") & (\n",
    "    F.col(\"event_time\") <= F.col(\"m_event_time\") + F.expr(f\"INTERVAL {JOIN_LOOKBACK}\")\n",
    ")\n",
    "\n",
    "# Perform the Join\n",
    "df_beta_calc = df.join(df_market, join_condition, \"left_outer\")\n",
    "\n",
    "# Aggregate Beta\n",
    "df_beta_agg = (df_beta_calc\n",
    "    .groupBy(\n",
    "        F.col(\"s\"),\n",
    "        F.window(\"event_time\", WINDOW_DURATION, SLIDE_DURATION)\n",
    "    )\n",
    "    .agg(\n",
    "        # Covariance(Ticker, Market) / Variance(Market)\n",
    "        # We use NULLIF to safely handle 0 variance (flat market)\n",
    "        (F.covar_samp(\"changePercent\", \"m_return\") / F.nullif(F.var_samp(\"m_return\"), F.lit(0))).alias(\"calc_beta\")\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# --- 4. MERGE & COMPUTE WACC WITH QUALITY FLAGS ---\n",
    "# (This logic remains exactly the same as before)\n",
    "df_final = df_ticker_stats.join(\n",
    "    df_beta_agg,\n",
    "    on=[\"s\", \"window\"],\n",
    "    how=\"left_outer\" \n",
    ")\n",
    "\n",
    "df_final = df_final.withColumn(\n",
    "    \"beta_source\",\n",
    "    F.when(F.col(\"calc_beta\").isNotNull(), F.lit(\"VALID_CALC\"))\n",
    "     .otherwise(F.lit(\"DEFAULT_FALLBACK\"))\n",
    ").withColumn(\n",
    "    \"beta\",\n",
    "    F.coalesce(F.col(\"calc_beta\"), F.lit(1.0)) \n",
    ").withColumn(\n",
    "    \"cost_of_equity\",\n",
    "    F.lit(RISK_FREE_RATE) + (F.col(\"beta\") * F.lit(MARKET_PREMIUM))\n",
    ")\n",
    "# df_final.filter(col(\"beta_source\") == \"VALID_CALC\").show()\n",
    "# FILTER: Only write meaningful data to the Gold layer\n",
    "# We discard the \"Overnight\" data where the market was flat.\n",
    "df_result = df_final.filter(\n",
    "    F.col(\"beta_source\") == \"VALID_CALC\"\n",
    ").select(\n",
    "    \"window\", \"s\", \"close_price\", \"volatility\", \"momentum\", \"beta\", \"cost_of_equity\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa02e4c5-3902-4a12-8871-b9de55e06132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "is there any valid points in teh dataset ( to eliminate the source data dorruption theory ) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "883c84b6-c35f-44c2-b8e5-087b531696ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, avg, min, max, count, stddev\n",
    "\n",
    "# # Aggregate by Ticker to check stability\n",
    "# df_stats = df_result.groupBy(\"s\") \\\n",
    "#     .agg(\n",
    "#         avg(\"beta\").alias(\"avg_beta\"),\n",
    "#         stddev(\"beta\").alias(\"beta_volatility\"),\n",
    "#         min(\"beta\").alias(\"min_beta\"),\n",
    "#         max(\"beta\").alias(\"max_beta\"),\n",
    "#         count(\"beta\").alias(\"windows_count\")\n",
    "#     ) \\\n",
    "#     .orderBy(\"s\")\n",
    "\n",
    "# df_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6656465-5c3c-4098-be24-e71e56e18f1c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "* f"
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# SELECT \n",
    "#     T.s AS ticker,\n",
    "    \n",
    "#     -- 1. BETA CALCULATION WITH SAFE DIVISION\n",
    "#     -- Logic: If Variance is 0 (or null), NULLIF returns NULL. \n",
    "#     --        Then COALESCE converts that NULL into 0.\n",
    "#     COALESCE(\n",
    "#         covar_samp(T.changePercent, M.changePercent) / NULLIF(var_samp(M.changePercent), 0), \n",
    "#         0\n",
    "#     ) as calculated_beta,\n",
    "\n",
    "#     -- 2. Debug Info\n",
    "#     count(*) as data_points_used\n",
    "\n",
    "# FROM try_calc_beta AS T \n",
    "# LEFT JOIN try_calc_beta AS M\n",
    "#   ON M.event_time >= T.event_time - INTERVAL 1 HOUR\n",
    "#   AND M.event_time <= T.event_time + INTERVAL 1 HOUR\n",
    "\n",
    "# WHERE \n",
    "#     M.s = 'SPY' \n",
    "#     AND T.s != 'SPY'\n",
    "\n",
    "# GROUP BY T.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "577ab284-d168-4c6a-9e71-70e90d6b26b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "after we confirmed that the data source is valid we are suspecting the window interval is the reponsible on the null beta we did compare window from 2H to 7H teh result was correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49e980e4-7c78-43c7-b6ce-52c779b9fbb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# WITH \n",
    "# -- 1. TEST 2-HOUR WINDOW (The \"Noise\" Zone)\n",
    "# beta_2h AS (\n",
    "#     SELECT \n",
    "#         T.s as ticker,\n",
    "#         COALESCE(\n",
    "#             covar_samp(T.changePercent, M.changePercent) / NULLIF(var_samp(M.changePercent), 0), \n",
    "#             0\n",
    "#         ) as beta_val\n",
    "#     FROM try_calc_beta T \n",
    "#     LEFT JOIN try_calc_beta M \n",
    "#       -- +/- 60 Minutes = 2 Hours Total\n",
    "#       ON M.event_time BETWEEN T.event_time - INTERVAL 60 MINUTES AND T.event_time + INTERVAL 60 MINUTES\n",
    "#     WHERE M.s = 'SPY' AND T.s != 'SPY'\n",
    "#     GROUP BY T.s\n",
    "# ),\n",
    "\n",
    "# -- 2. TEST 3-HOUR WINDOW (The \"Signal\" Zone?)\n",
    "# beta_3h AS (\n",
    "#     SELECT \n",
    "#         T.s as ticker,\n",
    "#         COALESCE(\n",
    "#             covar_samp(T.changePercent, M.changePercent) / NULLIF(var_samp(M.changePercent), 0), \n",
    "#             0\n",
    "#         ) as beta_val\n",
    "#     FROM try_calc_beta T \n",
    "#     LEFT JOIN try_calc_beta M \n",
    "#       -- +/- 90 Minutes = 3 Hours Total\n",
    "#       ON M.event_time BETWEEN T.event_time - INTERVAL 90 MINUTES AND T.event_time + INTERVAL 90 MINUTES\n",
    "#     WHERE M.s = 'SPY' AND T.s != 'SPY'\n",
    "#     GROUP BY T.s\n",
    "# ),\n",
    "\n",
    "# -- 3. TEST 4-HOUR WINDOW\n",
    "# beta_4h AS (\n",
    "#     SELECT \n",
    "#         T.s as ticker,\n",
    "#         COALESCE(\n",
    "#             covar_samp(T.changePercent, M.changePercent) / NULLIF(var_samp(M.changePercent), 0), \n",
    "#             0\n",
    "#         ) as beta_val\n",
    "#     FROM try_calc_beta T \n",
    "#     LEFT JOIN try_calc_beta M \n",
    "#       -- +/- 120 Minutes = 4 Hours Total\n",
    "#       ON M.event_time BETWEEN T.event_time - INTERVAL 120 MINUTES AND T.event_time + INTERVAL 120 MINUTES\n",
    "#     WHERE M.s = 'SPY' AND T.s != 'SPY'\n",
    "#     GROUP BY T.s\n",
    "# ),\n",
    "\n",
    "# -- 4. TEST 5-HOUR WINDOW\n",
    "# beta_5h AS (\n",
    "#     SELECT \n",
    "#         T.s as ticker,\n",
    "#         COALESCE(\n",
    "#             covar_samp(T.changePercent, M.changePercent) / NULLIF(var_samp(M.changePercent), 0), \n",
    "#             0\n",
    "#         ) as beta_val\n",
    "#     FROM try_calc_beta T \n",
    "#     LEFT JOIN try_calc_beta M \n",
    "#       -- +/- 150 Minutes = 5 Hours Total\n",
    "#       ON M.event_time BETWEEN T.event_time - INTERVAL 150 MINUTES AND T.event_time + INTERVAL 150 MINUTES\n",
    "#     WHERE M.s = 'SPY' AND T.s != 'SPY'\n",
    "#     GROUP BY T.s\n",
    "# )\n",
    "\n",
    "# -- 5. THE FINAL COMPARISON\n",
    "# SELECT \n",
    "#     A.ticker,\n",
    "    \n",
    "#     -- 2 Hours (Expected: 0)\n",
    "#     ROUND(A.beta_val, 3) as beta_2h,\n",
    "    \n",
    "#     -- 3 Hours (Expected: Breaking Point / Jump)\n",
    "#     ROUND(B.beta_val, 3) as beta_3h,\n",
    "    \n",
    "#     -- 4 Hours (Expected: Stable)\n",
    "#     ROUND(C.beta_val, 3) as beta_4h,\n",
    "    \n",
    "#     -- 5 Hours (Expected: Stable)\n",
    "#     ROUND(D.beta_val, 3) as beta_5h,\n",
    "    \n",
    "#     -- Visual Confirmation of the Jump\n",
    "#     CASE \n",
    "#         WHEN A.beta_val = 0 AND B.beta_val != 0 THEN 'BREAKING POINT'\n",
    "#         ELSE 'STABLE'\n",
    "#     END as status\n",
    "\n",
    "# FROM beta_2h A\n",
    "# JOIN beta_3h B ON A.ticker = B.ticker\n",
    "# JOIN beta_4h C ON A.ticker = C.ticker\n",
    "# JOIN beta_5h D ON A.ticker = D.ticker\n",
    "# ORDER BY A.ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "723e4776-e832-4fb6-bd72-d1060634b1ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This initializes the table.\n",
    "# df_result.write \\\n",
    "#     .format(\"delta\") \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .option(\"overwriteSchema\", \"true\") \\\n",
    "#     .save(destination_path)\n",
    "\n",
    "# print(\"Batch Backfill Complete. Table Initialized.\")\n",
    "\n",
    "# --- 3. WRITE STREAM ---\n",
    "df_result.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start(destination_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "gold_ticker_agg",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
