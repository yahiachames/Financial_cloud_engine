{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "4cbf5bd0-a1bc-49f7-af63-4ca7924db9a5",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "from datetime import datetime , timedelta\n",
                "from pyspark.sql import Window\n",
                "from pyspark.sql.functions import col, lit, input_file_name, current_timestamp,regexp_extract, to_date , explode , from_json\n",
                "from pyspark.sql.types import ArrayType , StringType , StructField , StructType\n",
                "dbutils.widgets.text(\"start_date\", datetime.now().strftime(\"%Y-%m-%d\"),\"Start date\")\n",
                "dbutils.widgets.text(\"end_date\", datetime.now().strftime(\"%Y-%m-%d\"),\"End date\")\n",
                "\n",
                "dbutils.widgets.text(\"mode\", \"INCREMENTAL\",\"mode\")\n",
                "\n",
                "start_date_str = dbutils.widgets.get(\"start_date\")\n",
                "end_date_str = dbutils.widgets.get(\"end_date\")\n",
                "mode = dbutils.widgets.get(\"mode\")\n",
                "\n",
                "date_format = \"%Y-%m-%d\"\n",
                "\n",
                "start_date = datetime.strptime(start_date_str, date_format).date()\n",
                "end_date = datetime.strptime(end_date_str, date_format).date()\n",
                "\n",
                "if start_date > end_date:\n",
                "    raise ValueError(f\"CRITICAL CONFIG ERROR: Start Date ({start_date}) is after End Date ({end_date}). Please check your parameters.\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "eff34b5c-9c0a-46dd-aebf-b3fe2cb294e6",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "ACCESS_KEY = dbutils.secrets.get(scope = \"ticker\", key = \"access_key\")\n",
                "SECRET_KEY = dbutils.secrets.get(scope = \"ticker\", key = \"secret_key\")\n",
                "SESSION_TOKEN = dbutils.secrets.get(scope = \"ticker\", key = \"session_key\")\n",
                "\n",
                "temp_ak = dbutils.jobs.taskValues.get(taskKey=\"Init_Auth\", key=\"temp_ak\", debugValue=\"debug-key\")\n",
                "temp_sk = dbutils.jobs.taskValues.get(taskKey=\"Init_Auth\", key=\"temp_sk\", debugValue=\"debug-secret\")\n",
                "temp_token = dbutils.jobs.taskValues.get(taskKey=\"Init_Auth\", key=\"temp_token\", debugValue=\"debug-token\")\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "0b51c378-7208-42a2-ba24-614451bfca3f",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "# Architecture Overview: Distributed Quality Gate & Environment Constraints\n",
                "\n",
                "## 1. Environment Constraints\n",
                "This pipeline operates under strict environmental constraints:\n",
                "- **No Instance Profiles**: Unable to assign IAM roles directly to clusters.\n",
                "- **Shared Compute**: Restricted access to cluster-level configurations (`spark.conf.set`) and prohibited mount points.\n",
                "- **Legacy Catalog**: Reliance on `hive_metastore` due to Unity Catalog absence.\n",
                "\n",
                "## 2. Strategic Solution: Session-Based Access\n",
                "To bypass these limitations, we implement **Session-Based Authentication**:\n",
                "- **Mechanism**: AWS credentials (ID/Secret/Token) are injected securely into the options of individual Reader and Writer objects.\n",
                "- **Outcome**: This allows direct, isolated S3 access (`s3a://`) without requiring cluster-level privileges.\n",
                "\n",
                "---\n",
                "\n",
                "## 3. Distributed Quality Gate Implementation\n",
                "\n",
                "We implement a **Distributed Validation Framework** to ensure data quality at scale:\n",
                "1.  **Contract Definition**: A reference SQL schema defines the expected types and constraints.\n",
                "2.  **Alignment**: Incoming data is cast to match the contract, with missing columns filled as NULL.\n",
                "3.  **Validation Logic**: Row-level checks identify type mismatches or constraint violations.\n",
                "4.  **Routing**: \n",
                "    - **Valid Rows** -> Silver Layer (Idempotent Merge)\n",
                "    - **Invalid Rows** -> Quarantine Layer (Append Only for audit)\n",
                "\n",
                "---\n",
                "\n",
                "### A. Setup & Schema Definition\n",
                "```python\n",
                "from pyspark.sql import DataFrame\n",
                "from pyspark.sql.types import StructType\n",
                "from pyspark.sql.functions import col, lit, when, concat_ws\n",
                "from delta.tables import *\n",
                "\n",
                "# 1. Configuration\n",
                "ACCESS_KEY = dbutils.secrets.get(scope=\"ticker\", key=\"access_key\")\n",
                "SECRET_KEY = dbutils.secrets.get(scope=\"ticker\", key=\"secret_key\")\n",
                "SESSION_TOKEN = dbutils.secrets.get(scope=\"ticker\", key=\"session_key\")\n",
                "\n",
                "# 2. Define the \"Contract\" (Target Schema) using SQL\n",
                "# We use a managed table just to hold the definition.\n",
                "spark.sql(\"\"\"\n",
                "CREATE TABLE IF NOT EXISTS company_financials_schema_holder (\n",
                "    date DATE NOT NULL,\n",
                "    symbol STRING NOT NULL,\n",
                "    revenue BIGINT NOT NULL,\n",
                "    fiscalYear INT,\n",
                "    eps DECIMAL(10, 4),\n",
                "    -- ... add all other columns ...\n",
                "    reportedCurrency STRING\n",
                ") USING DELTA\n",
                "\"\"\")\n",
                "\n",
                "target_schema = spark.table(\"company_financials_schema_holder\").schema"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "0def1f60-07a4-4eb5-a35c-40906b7a5120",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "# 1. LOAD MASTER SCHEMA\n",
                "target_schema = spark.table(\"company_financials_master_def\").schema\n",
                "base_path = \"s3a://mzon-to-databricks-5482/bronze/source=fmp/\"\n",
                "# 1. Always read from the Base Path (The Root)\n",
                "# Delta will automatically look at _delta_log to find the files.\n",
                "df_bronze_raw = (spark.read\n",
                "      .format(\"delta\")\n",
                "       .option(\"fs.s3a.access.key\", temp_ak)\n",
                "      .option(\"fs.s3a.secret.key\", temp_sk)\n",
                "      .option(\"fs.s3a.session.token\", temp_token)\n",
                "      .option(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
                "      .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
                "      .option(\"mode\" , \"PERMESSIVE\")\n",
                "      .option(\"dateFormat\", \"yyyy-MM-dd\") \n",
                "      .option(\"columnNameOfCorruptRecord\" , \"_rescued_data\")\n",
                "      .load(base_path)  # <--- No wildcards, no date=... loops\n",
                ")\n",
                "\n",
                "# 2. Apply \"Pushdown Predicate\" (The Filter)\n",
                "# Spark sends this logic to the Delta Log BEFORE reading data.\n",
                "if mode == \"INCREMENTAL\":\n",
                "    print(f\"Filtering for range: {start_date} to {end_date}\")\n",
                "    df_bronze = df_bronze_raw.filter(\n",
                "        (col(\"date\") >= lit(start_date)) & \n",
                "        (col(\"date\") <= lit(end_date))\n",
                "    )\n",
                "else:\n",
                "    # FULL_RELOAD: Just take everything\n",
                "    print(\"Full Reload: Reading all history.\")\n",
                "    df_bronze = df_bronze_raw\n",
                "\n",
                "# 3. Verify\n",
                "df_bronze.printSchema()\n",
                "print(f\"Row Count: {df_bronze.count()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "86f40ce0-15b1-44a1-9ec4-2b967d99a6a8",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "def schema_generator(schema):\n",
                "    schema_modified = schema.add(\"_corrupt_record\", StringType(), True)\n",
                "    json_schema = ArrayType(schema_modified)\n",
                "    return schema_modified , json_schema"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "658b3c2a-8ed4-4247-957a-69a5323cbf57",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "\n",
                "bronze_schema = spark.table(\"bronze_schema_holder\").schema\n",
                "bronze_schema_modified, bronze_schema_json = schema_generator(bronze_schema)\n",
                "\n",
                "income_statement_schema = spark.table(\"income_staement_silver_schema_holder\").schema\n",
                "income_statement_schema_modified, income_statement_schema_json = schema_generator(income_statement_schema)\n",
                "\n",
                "balance_sheet_statement_schema = spark.table(\"balance_sheet_staement_silver_schema_holder\").schema\n",
                "balance_sheet_statement_schema_modified, balance_sheet_statement_schema_json = schema_generator(balance_sheet_statement_schema)\n",
                "\n",
                "cashflow_statement_schema = spark.table(\"cash_flow_staement_silver_schema_holder\").schema\n",
                "cashflow_statement_schema_modified, cashflow_statement_schema_json = schema_generator(cashflow_statement_schema)\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "8af33d30-11d3-4912-8478-d8214858258d",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import col, lit, when, concat_ws,row_number\n",
                "from pyspark.sql import DataFrame\n",
                "from pyspark.sql.types import StructType\n",
                "\n",
                "def align_and_validate_strict(df: DataFrame, target_schema: StructType):\n",
                "    \"\"\"\n",
                "    STRICT VERSION:\n",
                "    - If a column is missing from the input DF, the row is marked BAD (Quarantined).\n",
                "    - If a column exists but has bad data (Type mismatch), the row is marked BAD.\n",
                "    \"\"\"\n",
                "    existing_cols = df.columns\n",
                "    selected_cols = []\n",
                "\n",
                "    for field in target_schema:\n",
                "        if field.name in existing_cols:\n",
                "            selected_cols.append(field.name)\n",
                "        else:\n",
                "            selected_cols.append(lit(None).try_cast(field.dataType).alias(field.name))\n",
                "\n",
                "    df_aligned = df.select(*selected_cols)\n",
                "    row_validation = []\n",
                "    for field in target_schema:\n",
                "        col_name = field.name\n",
                "        col_type = field.dataType\n",
                "        is_nullable = field.nullable\n",
                "        if not is_nullable:\n",
                "            is_valid_rule = col(col_name).isNotNull()\n",
                "        err_ms = when(~is_valid_rule , lit(col_name)).otherwise(lit(None))\n",
                "        row_validation.append(err_ms)\n",
                "\n",
                "    df_scored = df_aligned.withColumn(\"_failed_cols\" , concat_ws(\",\",*row_validation))\n",
                "    return df_scored\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "211fc53c-b68d-4ee3-a5c9-b230dc70f7d8",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "The function `DeltaTable.isDeltaTable(spark, path)` creates a fresh connection to S3 to check for the `_delta_log` folder.\n",
                "\n",
                "**The Problem**: This new connection does not know about the AWS keys (ACCESS_KEY, etc.) injected into the reader. It attempts an anonymous connection and is rejected by AWS.\n",
                "\n",
                "**The Constraint**: On a Shared Cluster, global keys (`spark.conf.set`) are banned, making DeltaTable utilities effectively absent."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "d4ebd60e-0f7e-40d1-a1f4-08f621becb27",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "def parse_df(df,json_schema,statement):\n",
                "    \n",
                "    #handle icome statement normalization\n",
                "    df = (df.withColumn(\"statement_type\", lit(statement))\n",
                "    .drop(statement)\n",
                "   .withColumn(\"fiscal_year_norm\", explode(from_json(col(\"value\") , json_schema,options={\"mode\": \"PERMISSIVE\", \"columnNameOfCorruptRecord\": \"_corrupt_record\"})) ) ) \n",
                "    df = df.select(col(\"ingestion_timestamp\"),col(\"source_file\") , col(\"fiscal_year_norm.*\"))\n",
                "    return df\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "a8efdce8-a2aa-4aaa-b9c6-1f89c341f6fc",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "def clean_df(df):\n",
                "    df_deduplicated = df\n",
                "    window_spec = Window.partitionBy(\"symbol\",\"date\") \\\n",
                "                        .orderBy(col(\"ingestion_timestamp\").desc())\n",
                "    df_deduplicated = df_deduplicated.withColumn(\"_rank\", row_number().over(window_spec)) \\\n",
                "                                    .filter(col(\"_rank\") == \"1\") \\\n",
                "                                    .drop(\"_rank\")\n",
                "    return df_deduplicated\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "f126b358-5d31-406d-8ae9-747f92ed8aa1",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "def validate_df(df,schema):\n",
                "    df_evaluated = align_and_validate_strict(df, schema)\n",
                "    #PERSIST TABLE is not supported on serverless compute. SQLSTATE: 0A000\n",
                "    #df_evaluated.cache()\n",
                "    valid_records = df_evaluated.filter(col(\"_failed_cols\") == \"\").drop(\"_failed_cols\")\n",
                "    invalid_records = df_evaluated.filter((col(\"_failed_cols\") != \"\") | (col(\"_corrupt_record\").isNotNull()) )\n",
                "    # df_evaluated.unpersist()\n",
                "    return valid_records, invalid_records\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "1bb4408f-eff8-42b0-b617-fa1a72d0d2c4",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "df_bronze_valid_records, df_bronze_invalid_records = validate_df(df_bronze,bronze_schema_modified)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "a4fcb1b1-8c36-45c3-aefc-5744d96a0c1f",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "\n",
                "df_income_statement =parse_df(df_bronze_valid_records.filter(\"statement_type = 'income-statement'\"),income_statement_schema_json,\"income-statement\")\n",
                "df_income_statement.show()\n",
                "df_income_statement = clean_df(df_income_statement)\n",
                "df_income_statement.show()\n",
                "df_income_statement_valid_records, df_income_statement_invalid_records = validate_df(df_income_statement,income_statement_schema_modified)\n",
                "df_income_statement_valid_records = df_income_statement_valid_records.drop('_corrupt_record')\n",
                "df_income_statement_valid_records.show()\n",
                "\n",
                "df_income_statement_invalid_records.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "86a3d856-046c-4181-a5a8-0721b6ae291d",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "df_balance_sheet =parse_df(df_bronze_valid_records.filter(\"statement_type = 'balance-sheet-statement'\"),balance_sheet_statement_schema_json,\"balance-sheet-statement\")\n",
                "df_balance_sheet.show()\n",
                "df_balance_sheet = clean_df(df_balance_sheet)\n",
                "df_balance_sheet.show()\n",
                "df_balance_sheet_valid_records, df_balance_sheet_invalid_records = validate_df(df_balance_sheet,balance_sheet_statement_schema_modified)\n",
                "df_balance_sheet_valid_records = df_balance_sheet_valid_records.drop('_corrupt_record')\n",
                "df_balance_sheet_valid_records.show()\n",
                "df_balance_sheet_invalid_records.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "e0ff3176-10a7-4330-8bfb-d96a5053f85a",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "df_cashflow_statement =parse_df(df_bronze_valid_records.filter(\"statement_type = 'cash-flow-statement'\"),cashflow_statement_schema_json,\"cash-flow-statement\")\n",
                "df_cashflow_statement.show()\n",
                "df_cashflow_statement = clean_df(df_cashflow_statement)\n",
                "df_cashflow_statement.show()\n",
                "df_cashflow_statement_valid_records, df_cashflow_statement_invalid_records = validate_df(df_cashflow_statement,cashflow_statement_schema_modified)\n",
                "df_cashflow_statement_valid_records = df_cashflow_statement_valid_records.drop('_corrupt_record')\n",
                "df_cashflow_statement_valid_records.show()\n",
                "df_cashflow_statement_invalid_records.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "833eecee-0be3-4ec7-8dbb-8a00eafa6455",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "# from delta.tables import *\n",
                "# # 1. Define your paths and data\n",
                "# silver_path = \"s3a://mzon-to-databricks-5482/silver/income_statement/valid\"\n",
                "\n",
                "# if DeltaTable.isDeltaTable(spark, silver_path):\n",
                "#     # MERGE (Upsert)\n",
                "#     target_table = DeltaTable.forPath(spark, silver_path)\n",
                "#     (target_table.alias(\"target\")\n",
                "#         .merge(\n",
                "#             valid_records.alias(\"source\"), \n",
                "#             \"target.symbol = source.symbol AND target.date = source.date\"\n",
                "#         )\n",
                "#         .whenMatchedUpdateAll()\n",
                "#         .whenNotMatchedInsertAll()\n",
                "#         .execute()\n",
                "#     )\n",
                "# else:\n",
                "#     # INITIALIZE (Create)\n",
                "#     (valid_records.write\n",
                "#         .format(\"delta\")\n",
                "#         .mode(\"overwrite\") \n",
                "#         .partitionBy(\"date\")  # <--- CRITICAL: Partitioning Strategy\n",
                "#         .save(silver_path)\n",
                "#     )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "554f0480-9f8d-4a7f-97e0-88cf9b41b0e5",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "### Write Strategy: Credential-Injection Overwrite\n",
                "\n",
                "We use **Overwrite by Partition** with a `replaceWhere` condition. This achieves Idempotency while allowing us to explicitly pass the `fs.s3a.access.key` credentials in the `.write` options, bypassing the need for an Instance Profile."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "8670305b-03b0-405b-a35f-91fbbb27cf8c",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "def write_df(df,label,path,mode):\n",
                "    if mode == \"INCREMENTAL\" : \n",
                "            (df.write\n",
                "            .format(\"delta\")\n",
                "            .mode(\"overwrite\")\n",
                "            .partitionBy(\"date\")\n",
                "            # CRITICAL: This condition ensures we only overwrite the partitions present in the current batch\n",
                "            .option(\"replaceWhere\", f\"date >= '{start_date}' AND date <= '{end_date}'\")\n",
                "            # INJECT CREDENTIALS AGAIN (Required for the Writer)\n",
                "            .option(\"fs.s3a.access.key\", temp_ak)\n",
                "            .option(\"fs.s3a.secret.key\", temp_sk)\n",
                "            .option(\"fs.s3a.session.token\", temp_token)\n",
                "            .option(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
                "\n",
                "            .save(path + f\"/{label}\"))\n",
                "    else:\n",
                "        (df.write\n",
                "        .format(\"delta\")\n",
                "        .mode(\"overwrite\")\n",
                "        .partitionBy(\"date\")\n",
                "        # INJECT CREDENTIALS AGAIN (Required for the Writer)\n",
                "    .option(\"fs.s3a.access.key\", temp_ak)\n",
                "            .option(\"fs.s3a.secret.key\", temp_sk)\n",
                "            .option(\"fs.s3a.session.token\", temp_token)\n",
                "        .option(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
                "        .option(\"overwriteSchema\", \"true\")\n",
                "        .save(path + f\"/{label}\")\n",
                "    )\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "ad465dcc-770b-41d2-9688-7964164f2797",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "df_bronze_invalid_records.printSchema()\n",
                "df_bronze_invalid_records.show()\n",
                "bronze_path_invalid = \"s3a://mzon-to-databricks-5482/bronze\"\n",
                "write_df(df_bronze_invalid_records,\"invalid\",bronze_path_invalid,\"FULL\")\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "c0be4b4b-2988-4d4d-b08e-ec56558ace98",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "silver_path_valid = \"s3a://mzon-to-databricks-5482/silver/valid\"\n",
                "write_df(df_income_statement_valid_records,\"income_statement\",silver_path_valid,mode)\n",
                "write_df(df_balance_sheet_valid_records,\"balance_sheet\",silver_path_valid,mode)\n",
                "write_df(df_cashflow_statement_valid_records,\"cashflow_statement\",silver_path_valid,mode)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "f6bc86ab-1c99-476c-92fc-4ad3d630a974",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "silver_path_invalid = \"s3a://mzon-to-databricks-5482/silver/invalid\"\n",
                "write_df(df_income_statement_invalid_records,\"income_statement\",silver_path_invalid,\"FULL\")\n",
                "write_df(df_balance_sheet_invalid_records,\"balance_sheet\",silver_path_invalid,\"FULL\")\n",
                "write_df(df_cashflow_statement_invalid_records,\"cashflow_statement\",silver_path_invalid,\"FULL\")"
            ]
        }
    ],
    "metadata": {
        "application/vnd.databricks.v1+notebook": {
            "computePreferences": null,
            "dashboards": [],
            "environmentMetadata": {
                "base_environment": "",
                "environment_version": "4"
            },
            "inputWidgetPreferences": null,
            "language": "python",
            "notebookMetadata": {
                "mostRecentlyExecutedCommandWithImplicitDF": {
                    "commandId": 7400122869111745,
                    "dataframes": [
                        "_sqldf"
                    ]
                },
                "pythonIndentUnit": 4
            },
            "notebookName": "silver_financials_batch",
            "widgets": {
                "end_date": {
                    "currentValue": "2026-01-14",
                    "nuid": "3e3ccc72-64ca-4c2f-acee-a9b778d60a92",
                    "typedWidgetInfo": {
                        "autoCreated": false,
                        "defaultValue": "2026-01-14",
                        "label": "End date",
                        "name": "end_date",
                        "options": {
                            "widgetDisplayType": "Text",
                            "validationRegex": null
                        },
                        "parameterDataType": "String"
                    },
                    "widgetInfo": {
                        "widgetType": "text",
                        "defaultValue": "2026-01-14",
                        "label": "End date",
                        "name": "end_date",
                        "options": {
                            "widgetType": "text",
                            "autoCreated": null,
                            "validationRegex": null
                        }
                    }
                },
                "mode": {
                    "currentValue": "INCREMENTAL",
                    "nuid": "a47ce8d2-bdd3-4572-a4d5-620a4b711fc3",
                    "typedWidgetInfo": {
                        "autoCreated": false,
                        "defaultValue": "INCREMENTAL",
                        "label": "mode",
                        "name": "mode",
                        "options": {
                            "widgetDisplayType": "Text",
                            "validationRegex": null
                        },
                        "parameterDataType": "String"
                    },
                    "widgetInfo": {
                        "widgetType": "text",
                        "defaultValue": "INCREMENTAL",
                        "label": "mode",
                        "name": "mode",
                        "options": {
                            "widgetType": "text",
                            "autoCreated": null,
                            "validationRegex": null
                        }
                    }
                },
                "start_date": {
                    "currentValue": "2025-01-14",
                    "nuid": "52c48461-d4bc-4618-ac4b-8606cbd25cf8",
                    "typedWidgetInfo": {
                        "autoCreated": false,
                        "defaultValue": "2026-01-14",
                        "label": "Start date",
                        "name": "start_date",
                        "options": {
                            "widgetDisplayType": "Text",
                            "validationRegex": null
                        },
                        "parameterDataType": "String"
                    },
                    "widgetInfo": {
                        "widgetType": "text",
                        "defaultValue": "2026-01-14",
                        "label": "Start date",
                        "name": "start_date",
                        "options": {
                            "widgetType": "text",
                            "autoCreated": null,
                            "validationRegex": null
                        }
                    }
                }
            }
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}