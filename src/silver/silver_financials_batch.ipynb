{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cbf5bd0-a1bc-49f7-af63-4ca7924db9a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime , timedelta\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, lit, input_file_name, current_timestamp,regexp_extract, to_date , explode , from_json\n",
    "from pyspark.sql.types import ArrayType , StringType , StructField , StructType\n",
    "dbutils.widgets.text(\"start_date\", datetime.now().strftime(\"%Y-%m-%d\"),\"Start date\")\n",
    "dbutils.widgets.text(\"end_date\", datetime.now().strftime(\"%Y-%m-%d\"),\"End date\")\n",
    "\n",
    "dbutils.widgets.text(\"mode\", \"INCREMENTAL\",\"mode\")\n",
    "\n",
    "start_date_str = dbutils.widgets.get(\"start_date\")\n",
    "end_date_str = dbutils.widgets.get(\"end_date\")\n",
    "mode = dbutils.widgets.get(\"mode\")\n",
    "\n",
    "date_format = \"%Y-%m-%d\"\n",
    "\n",
    "start_date = datetime.strptime(start_date_str, date_format).date()\n",
    "end_date = datetime.strptime(end_date_str, date_format).date()\n",
    "\n",
    "if start_date > end_date:\n",
    "    raise ValueError(f\"CRITICAL CONFIG ERROR: Start Date ({start_date}) is after End Date ({end_date}). Please check your parameters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eff34b5c-9c0a-46dd-aebf-b3fe2cb294e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ACCESS_KEY = dbutils.secrets.get(scope = \"ticker\", key = \"access_key\")\n",
    "SECRET_KEY = dbutils.secrets.get(scope = \"ticker\", key = \"secret_key\")\n",
    "SESSION_TOKEN = dbutils.secrets.get(scope = \"ticker\", key = \"session_key\")\n",
    "\n",
    "temp_ak = dbutils.jobs.taskValues.get(taskKey=\"Init_Auth\", key=\"temp_ak\", debugValue=\"debug-key\")\n",
    "temp_sk = dbutils.jobs.taskValues.get(taskKey=\"Init_Auth\", key=\"temp_sk\", debugValue=\"debug-secret\")\n",
    "temp_token = dbutils.jobs.taskValues.get(taskKey=\"Init_Auth\", key=\"temp_token\", debugValue=\"debug-token\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b51c378-7208-42a2-ba24-614451bfca3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Architectural Review: PySpark Quality Gate in a Restricted Environment\n",
    "\n",
    "## 1. The Challenge: Operating in a \"Hostile\" Environment\n",
    "We faced a specific set of constraints common in restricted corporate environments or the Databricks Community Edition. These constraints dictated every architectural decision we made.\n",
    "\n",
    "### The Constraints\n",
    "| Constraint | Impact | Resulting Limitation |\n",
    "| :--- | :--- | :--- |\n",
    "| **No IAM Role AWS Educate** | Cannot create Instance Profiles or Unity Catalog Storage Credentials. | **No External Tables**: We cannot use `CREATE EXTERNAL TABLE` because we cannot authorize the storage location at the cluster level. |\n",
    "| **Shared Cluster** | Strict security isolation between users. | **Blocked Configs**: `spark.conf.set` and `dbutils.fs.mount` are banned to prevent credential leakage. |\n",
    "| **Legacy Catalog** | Unity Catalog is unavailable. | **`hive_metastore` Error**: We must use `spark_catalog` (the legacy default). |\n",
    "\n",
    "### The Strategic Decision: \"Session-Based Access\"\n",
    "Since we could not rely on the Cluster to manage authentication, we shifted authentication to the **Session Level**.\n",
    "* **Bypass:** We bypass the Cluster Manager entirely.\n",
    "* **Method:** We inject AWS keys securely into the `.option()` of every Reader and Writer.\n",
    "* **Result:** The Python process talks directly to S3 (`s3a://`), completely independent of the Cluster's lack of permissions.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Solution: \"Distributed Quality Gate\" Architecture\n",
    "\n",
    "We rejected the \"Unit Test\" approach (which crashes on Big Data) and built a **Distributed Validation Framework** that processes data in parallel on the Workers.\n",
    "\n",
    "### The Pipeline Flow\n",
    "1.  **Define Contract:** Create a dummy Managed Table in SQL to define the target schema (Types & Nullability).\n",
    "2.  **Align:** Python function forces input data to match the Contract (injecting `NULL` for missing columns).\n",
    "3.  **Validate:** Spark logic (`when/otherwise`) checks data types and constraints row-by-row.\n",
    "4.  **Split:**\n",
    "    * **Good Rows:** Sent to **Silver** (Idempotent Merge).\n",
    "    * **Bad Rows:** Sent to **Quarantine** (Append Only).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. The Master Script (Implementation)\n",
    "\n",
    "Below is the complete, robust code implementing the logic we developed.\n",
    "\n",
    "### A. Setup & Schema Definition\n",
    "```python\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import col, lit, when, concat_ws\n",
    "from delta.tables import *\n",
    "\n",
    "# 1. Configuration\n",
    "ACCESS_KEY = dbutils.secrets.get(scope=\"ticker\", key=\"access_key\")\n",
    "SECRET_KEY = dbutils.secrets.get(scope=\"ticker\", key=\"secret_key\")\n",
    "SESSION_TOKEN = dbutils.secrets.get(scope=\"ticker\", key=\"session_key\")\n",
    "\n",
    "# 2. Define the \"Contract\" (Target Schema) using SQL\n",
    "# We use a managed table just to hold the definition.\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS company_financials_schema_holder (\n",
    "    date DATE NOT NULL,\n",
    "    symbol STRING NOT NULL,\n",
    "    revenue BIGINT NOT NULL,\n",
    "    fiscalYear INT,\n",
    "    eps DECIMAL(10, 4),\n",
    "    -- ... add all other columns ...\n",
    "    reportedCurrency STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "target_schema = spark.table(\"company_financials_schema_holder\").schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0def1f60-07a4-4eb5-a35c-40906b7a5120",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. LOAD MASTER SCHEMA\n",
    "target_schema = spark.table(\"company_financials_master_def\").schema\n",
    "base_path = \"s3a://mzon-to-databricks-5482/bronze/source=fmp/\"\n",
    "# 1. Always read from the Base Path (The Root)\n",
    "# Delta will automatically look at _delta_log to find the files.\n",
    "df_bronze_raw = (spark.read\n",
    "      .format(\"delta\")\n",
    "       .option(\"fs.s3a.access.key\", temp_ak)\n",
    "      .option(\"fs.s3a.secret.key\", temp_sk)\n",
    "      .option(\"fs.s3a.session.token\", temp_token)\n",
    "      .option(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "      .option(\"mode\" , \"PERMESSIVE\")\n",
    "      .option(\"dateFormat\", \"yyyy-MM-dd\") \n",
    "      .option(\"columnNameOfCorruptRecord\" , \"_rescued_data\")\n",
    "      .load(base_path)  # <--- No wildcards, no date=... loops\n",
    ")\n",
    "\n",
    "# 2. Apply \"Pushdown Predicate\" (The Filter)\n",
    "# Spark sends this logic to the Delta Log BEFORE reading data.\n",
    "if mode == \"INCREMENTAL\":\n",
    "    print(f\"Filtering for range: {start_date} to {end_date}\")\n",
    "    df_bronze = df_bronze_raw.filter(\n",
    "        (col(\"date\") >= lit(start_date)) & \n",
    "        (col(\"date\") <= lit(end_date))\n",
    "    )\n",
    "else:\n",
    "    # FULL_RELOAD: Just take everything\n",
    "    print(\"Full Reload: Reading all history.\")\n",
    "    df_bronze = df_bronze_raw\n",
    "\n",
    "# 3. Verify\n",
    "df_bronze.printSchema()\n",
    "print(f\"Row Count: {df_bronze.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86f40ce0-15b1-44a1-9ec4-2b967d99a6a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def schema_generator(schema):\n",
    "    schema_modified = schema.add(\"_corrupt_record\", StringType(), True)\n",
    "    json_schema = ArrayType(schema_modified)\n",
    "    return schema_modified , json_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "658b3c2a-8ed4-4247-957a-69a5323cbf57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "bronze_schema = spark.table(\"bronze_schema_holder\").schema\n",
    "bronze_schema_modified, bronze_schema_json = schema_generator(bronze_schema)\n",
    "\n",
    "income_statement_schema = spark.table(\"income_staement_silver_schema_holder\").schema\n",
    "income_statement_schema_modified, income_statement_schema_json = schema_generator(income_statement_schema)\n",
    "\n",
    "balance_sheet_statement_schema = spark.table(\"balance_sheet_staement_silver_schema_holder\").schema\n",
    "balance_sheet_statement_schema_modified, balance_sheet_statement_schema_json = schema_generator(balance_sheet_statement_schema)\n",
    "\n",
    "cashflow_statement_schema = spark.table(\"cash_flow_staement_silver_schema_holder\").schema\n",
    "cashflow_statement_schema_modified, cashflow_statement_schema_json = schema_generator(cashflow_statement_schema)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8af33d30-11d3-4912-8478-d8214858258d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, when, concat_ws,row_number\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "def align_and_validate_strict(df: DataFrame, target_schema: StructType):\n",
    "    \"\"\"\n",
    "    STRICT VERSION:\n",
    "    - If a column is missing from the input DF, the row is marked BAD (Quarantined).\n",
    "    - If a column exists but has bad data (Type mismatch), the row is marked BAD.\n",
    "    \"\"\"\n",
    "    existing_cols = df.columns\n",
    "    selected_cols = []\n",
    "\n",
    "    for field in target_schema:\n",
    "        if field.name in existing_cols:\n",
    "            selected_cols.append(field.name)\n",
    "        else:\n",
    "            selected_cols.append(lit(None).try_cast(field.dataType).alias(field.name))\n",
    "\n",
    "    df_aligned = df.select(*selected_cols)\n",
    "    row_validation = []\n",
    "    for field in target_schema:\n",
    "        col_name = field.name\n",
    "        col_type = field.dataType\n",
    "        is_nullable = field.nullable\n",
    "        if not is_nullable:\n",
    "            is_valid_rule = col(col_name).isNotNull()\n",
    "        err_ms = when(~is_valid_rule , lit(col_name)).otherwise(lit(None))\n",
    "        row_validation.append(err_ms)\n",
    "\n",
    "    df_scored = df_aligned.withColumn(\"_failed_cols\" , concat_ws(\",\",*row_validation))\n",
    "    return df_scored\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "211fc53c-b68d-4ee3-a5c9-b230dc70f7d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "he function DeltaTable.isDeltaTable(spark, path) creates a fresh connection to S3 to check for the _delta_log folder.\n",
    "\n",
    "The Problem: This new connection does not know about the AWS keys (ACCESS_KEY, etc.) you injected into the df_bronze reader. It tries to connect anonymously and gets rejected by AWS.\n",
    "\n",
    "The Constraint: Because you are on a Shared Cluster, you cannot set these keys globally (spark.conf.set), so DeltaTable utilities are effectively banned for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4ebd60e-0f7e-40d1-a1f4-08f621becb27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parse_df(df,json_schema,statement):\n",
    "    \n",
    "    #handle icome statement normalization\n",
    "    df = (df.withColumn(\"statement_type\", lit(statement))\n",
    "    .drop(statement)\n",
    "   .withColumn(\"fiscal_year_norm\", explode(from_json(col(\"value\") , json_schema,options={\"mode\": \"PERMISSIVE\", \"columnNameOfCorruptRecord\": \"_corrupt_record\"})) ) ) \n",
    "    df = df.select(col(\"ingestion_timestamp\"),col(\"source_file\") , col(\"fiscal_year_norm.*\"))\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8efdce8-a2aa-4aaa-b9c6-1f89c341f6fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    df_deduplicated = df\n",
    "    window_spec = Window.partitionBy(\"symbol\",\"date\") \\\n",
    "                        .orderBy(col(\"ingestion_timestamp\").desc())\n",
    "    df_deduplicated = df_deduplicated.withColumn(\"_rank\", row_number().over(window_spec)) \\\n",
    "                                    .filter(col(\"_rank\") == \"1\") \\\n",
    "                                    .drop(\"_rank\")\n",
    "    return df_deduplicated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f126b358-5d31-406d-8ae9-747f92ed8aa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def validate_df(df,schema):\n",
    "    df_evaluated = align_and_validate_strict(df, schema)\n",
    "    #PERSIST TABLE is not supported on serverless compute. SQLSTATE: 0A000\n",
    "    #df_evaluated.cache()\n",
    "    valid_records = df_evaluated.filter(col(\"_failed_cols\") == \"\").drop(\"_failed_cols\")\n",
    "    invalid_records = df_evaluated.filter((col(\"_failed_cols\") != \"\") | (col(\"_corrupt_record\").isNotNull()) )\n",
    "    # df_evaluated.unpersist()\n",
    "    return valid_records, invalid_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bb4408f-eff8-42b0-b617-fa1a72d0d2c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze_valid_records, df_bronze_invalid_records = validate_df(df_bronze,bronze_schema_modified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4fcb1b1-8c36-45c3-aefc-5744d96a0c1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_income_statement =parse_df(df_bronze_valid_records.filter(\"statement_type = 'income-statement'\"),income_statement_schema_json,\"income-statement\")\n",
    "df_income_statement.show()\n",
    "df_income_statement = clean_df(df_income_statement)\n",
    "df_income_statement.show()\n",
    "df_income_statement_valid_records, df_income_statement_invalid_records = validate_df(df_income_statement,income_statement_schema_modified)\n",
    "df_income_statement_valid_records = df_income_statement_valid_records.drop('_corrupt_record')\n",
    "df_income_statement_valid_records.show()\n",
    "\n",
    "df_income_statement_invalid_records.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86a3d856-046c-4181-a5a8-0721b6ae291d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_balance_sheet =parse_df(df_bronze_valid_records.filter(\"statement_type = 'balance-sheet-statement'\"),balance_sheet_statement_schema_json,\"balance-sheet-statement\")\n",
    "df_balance_sheet.show()\n",
    "df_balance_sheet = clean_df(df_balance_sheet)\n",
    "df_balance_sheet.show()\n",
    "df_balance_sheet_valid_records, df_balance_sheet_invalid_records = validate_df(df_balance_sheet,balance_sheet_statement_schema_modified)\n",
    "df_balance_sheet_valid_records = df_balance_sheet_valid_records.drop('_corrupt_record')\n",
    "df_balance_sheet_valid_records.show()\n",
    "df_balance_sheet_invalid_records.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0ff3176-10a7-4330-8bfb-d96a5053f85a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cashflow_statement =parse_df(df_bronze_valid_records.filter(\"statement_type = 'cash-flow-statement'\"),cashflow_statement_schema_json,\"cash-flow-statement\")\n",
    "df_cashflow_statement.show()\n",
    "df_cashflow_statement = clean_df(df_cashflow_statement)\n",
    "df_cashflow_statement.show()\n",
    "df_cashflow_statement_valid_records, df_cashflow_statement_invalid_records = validate_df(df_cashflow_statement,cashflow_statement_schema_modified)\n",
    "df_cashflow_statement_valid_records = df_cashflow_statement_valid_records.drop('_corrupt_record')\n",
    "df_cashflow_statement_valid_records.show()\n",
    "df_cashflow_statement_invalid_records.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "833eecee-0be3-4ec7-8dbb-8a00eafa6455",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from delta.tables import *\n",
    "# # 1. Define your paths and data\n",
    "# silver_path = \"s3a://mzon-to-databricks-5482/silver/income_statement/valid\"\n",
    "\n",
    "# if DeltaTable.isDeltaTable(spark, silver_path):\n",
    "#     # MERGE (Upsert)\n",
    "#     target_table = DeltaTable.forPath(spark, silver_path)\n",
    "#     (target_table.alias(\"target\")\n",
    "#         .merge(\n",
    "#             valid_records.alias(\"source\"), \n",
    "#             \"target.symbol = source.symbol AND target.date = source.date\"\n",
    "#         )\n",
    "#         .whenMatchedUpdateAll()\n",
    "#         .whenNotMatchedInsertAll()\n",
    "#         .execute()\n",
    "#     )\n",
    "# else:\n",
    "#     # INITIALIZE (Create)\n",
    "#     (valid_records.write\n",
    "#         .format(\"delta\")\n",
    "#         .mode(\"overwrite\") \n",
    "#         .partitionBy(\"date\")  # <--- CRITICAL: Partitioning Strategy\n",
    "#         .save(silver_path)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "554f0480-9f8d-4a7f-97e0-88cf9b41b0e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The Solution: \"Blind Write\" with replaceWhere\n",
    "We must abandon the MERGE command because it requires reading the table first (which you can't authorize).\n",
    "\n",
    "Instead, we will use Overwrite by Partition with a replaceWhere condition. This achieves the same goal (Idempotency) but allows us to pass the credentials directly in the .write options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8670305b-03b0-405b-a35f-91fbbb27cf8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_df(df,label,path,mode):\n",
    "    if mode == \"INCREMENTAL\" : \n",
    "            (df.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .partitionBy(\"date\")\n",
    "            # CRITICAL: This condition ensures we only overwrite the partitions present in the current batch\n",
    "            .option(\"replaceWhere\", f\"date >= '{start_date}' AND date <= '{end_date}'\")\n",
    "            # INJECT CREDENTIALS AGAIN (Required for the Writer)\n",
    "            .option(\"fs.s3a.access.key\", temp_ak)\n",
    "            .option(\"fs.s3a.secret.key\", temp_sk)\n",
    "            .option(\"fs.s3a.session.token\", temp_token)\n",
    "            .option(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "\n",
    "            .save(path + f\"/{label}\"))\n",
    "    else:\n",
    "        (df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"date\")\n",
    "        # INJECT CREDENTIALS AGAIN (Required for the Writer)\n",
    "    .option(\"fs.s3a.access.key\", temp_ak)\n",
    "            .option(\"fs.s3a.secret.key\", temp_sk)\n",
    "            .option(\"fs.s3a.session.token\", temp_token)\n",
    "        .option(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .save(path + f\"/{label}\")\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad465dcc-770b-41d2-9688-7964164f2797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze_invalid_records.printSchema()\n",
    "df_bronze_invalid_records.show()\n",
    "bronze_path_invalid = \"s3a://mzon-to-databricks-5482/bronze\"\n",
    "write_df(df_bronze_invalid_records,\"invalid\",bronze_path_invalid,\"FULL\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0be4b4b-2988-4d4d-b08e-ec56558ace98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_path_valid = \"s3a://mzon-to-databricks-5482/silver/valid\"\n",
    "write_df(df_income_statement_valid_records,\"income_statement\",silver_path_valid,mode)\n",
    "write_df(df_balance_sheet_valid_records,\"balance_sheet\",silver_path_valid,mode)\n",
    "write_df(df_cashflow_statement_valid_records,\"cashflow_statement\",silver_path_valid,mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6bc86ab-1c99-476c-92fc-4ad3d630a974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_path_invalid = \"s3a://mzon-to-databricks-5482/silver/invalid\"\n",
    "write_df(df_income_statement_invalid_records,\"income_statement\",silver_path_invalid,\"FULL\")\n",
    "write_df(df_balance_sheet_invalid_records,\"balance_sheet\",silver_path_invalid,\"FULL\")\n",
    "write_df(df_cashflow_statement_invalid_records,\"cashflow_statement\",silver_path_invalid,\"FULL\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7400122869111745,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_financials_batch.ipynb",
   "widgets": {
    "end_date": {
     "currentValue": "2025-12-25",
     "nuid": "3e3ccc72-64ca-4c2f-acee-a9b778d60a92",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2026-01-10",
      "label": "End date",
      "name": "end_date",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "2026-01-10",
      "label": "End date",
      "name": "end_date",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "mode": {
     "currentValue": "FULL",
     "nuid": "a47ce8d2-bdd3-4572-a4d5-620a4b711fc3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "INCREMENTAL",
      "label": "mode",
      "name": "mode",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "INCREMENTAL",
      "label": "mode",
      "name": "mode",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "start_date": {
     "currentValue": "2025-12-25",
     "nuid": "52c48461-d4bc-4618-ac4b-8606cbd25cf8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2026-01-10",
      "label": "Start date",
      "name": "start_date",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "2026-01-10",
      "label": "Start date",
      "name": "start_date",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
