{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d8ac713-7b58-443d-b267-983551d65f1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import ArrayType , StringType , StructField , StructType , LongType , DoubleType , IntegerType , BooleanType\n",
    "from pyspark.sql.window import Window\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8b34013-0e62-4334-9d47-d98a1032450e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_path = \"/Volumes/workspace/default/storage/bronze/ticker_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "860a92f6-c1f3-40ac-a6db-fd4a995f25a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# providing a starting version\n",
    "df = spark.readStream.format(\"delta\") \\\n",
    "  .option(\"startingVersion\", \"0\") \\\n",
    "  .load(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d3f84d9-8cc9-4145-bdd1-fb489f974185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a2c0e25-d105-4af8-8b34-e94989eea3d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"s\", StringType(), True),\n",
    "     StructField(\"currentPrice\", DoubleType(), True),\n",
    "     StructField(\"change\", DoubleType(), True),\n",
    "     StructField(\"changePercent\", DoubleType(), True),\n",
    "     StructField(\"high\", DoubleType(), True),\n",
    "     StructField(\"low\", DoubleType(), True),\n",
    "     StructField(\"open\", DoubleType(), True),\n",
    "     StructField(\"previousClose\", DoubleType(), True),\n",
    "         StructField(\"timestamp\", LongType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba8319de-58b6-4dc1-a711-4c652376b6de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"topic\", \"partition\", \"offset\")\n",
    "df = df.withColumn(\"ticker\" , F.from_json(F.col(\"value\"), schema,options={\"mode\": \"PERMISSIVE\", \"columnNameOfCorruptRecord\": \"_corrupt_record\"}))\n",
    "df =df.select(\"ticker.*\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f60b0947-f73b-4033-83b0-5a4c6e933d6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"event_time\" , (F.col(\"timestamp\")/1000).cast(\"timestamp\"))\n",
    "df = df.filter(\"changePercent IS NOT NULL AND changePercent != 0\")\n",
    "\n",
    "\n",
    "df_market = df.filter(\"s = 'SPY'\") \\\n",
    "              .selectExpr(\"changePercent as m_return\" , \"CAST(event_time as TIMESTAMP) as m_event_time\") \\\n",
    "                   .withColumn(\"m_join_key\", F.lit(1)) \\\n",
    "              .withWatermark(\"m_event_time\", \"2 minutes\")\n",
    "\n",
    "df_tickers = df.filter(\"s != 'SPY'\") \\\n",
    "               .withColumn(\"join_key\", F.lit(1)) \\\n",
    "                .withWatermark(\"event_time\", \"2 minutes\")\n",
    "\n",
    "join_condition = (\n",
    "    F.col(\"m_join_key\") == F.col(\"join_key\")\n",
    ") & (\n",
    "    F.col(\"event_time\") >= F.col(\"m_event_time\") - F.expr(\"INTERVAL 2 MINUTES\")\n",
    ") & (\n",
    "    F.col(\"event_time\") <= F.col(\"m_event_time\") + F.expr(\"INTERVAL 2 MINUTES\")\n",
    ")\n",
    "\n",
    "df = df_tickers.join(df_market, join_condition , \"inner\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ea79a3d-75f2-46f8-bae5-730b6168db4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_result = (df.groupBy(\n",
    "#                 F.col(\"s\"),\n",
    "#                 F.window(\"event_time\", \"10 minutes\")\n",
    "#             )\n",
    "#             .agg(\n",
    "#                 # The Beta Logic\n",
    "#                 (F.covar_samp(\"changePercent\", \"m_return\") / F.var_samp(\"m_return\")).alias(\"beta\"),\n",
    "                \n",
    "#                 # The \"Missing\" Columns - We must aggregate them\n",
    "#                 F.first(\"open\").alias(\"open_price\"),\n",
    "#                 F.max(\"high\").alias(\"high_price\"),\n",
    "#                 F.min(\"low\").alias(\"low_price\"),\n",
    "#                 F.last(\"currentPrice\").alias(\"close_price\"),\n",
    "#                 F.count(\"s\").alias(\"tick_count\")\n",
    "#             ))\n",
    "\n",
    "df_result = df.groupBy(\"s\", F.window(\"event_time\", \"10 minutes\")) \\\n",
    "                     .agg(F.count(\"*\").alias(\"match_count\"))\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c21ec44b-09df-4cb2-b460-4f888e18d1f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ACCESS_KEY = dbutils.secrets.get(scope = \"ticker\", key = \"access_key\")\n",
    "SECRET_KEY = dbutils.secrets.get(scope = \"ticker\", key = \"secret_key\")\n",
    "SESSION_TOKEN = dbutils.secrets.get(scope = \"ticker\", key = \"session_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5145c30d-58ce-4185-aa80-a2ea1b023e35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "BUCKET = \"mzon-to-databricks-5482\"\n",
    "destination_path = \"/Volumes/workspace/default/storage/silver/ticker_data_v3\"\n",
    "# --- 2. THE FIX: PATH MUST INCLUDE THE VOLUME NAME ---\n",
    "# Path format: /Volumes/<catalog>/<schema>/<VOLUME_NAME>/<folder>\n",
    "# We added 'storage' because that is the volume we just created in SQL.\n",
    "checkpoint_path = \"/Volumes/workspace/default/storage/checkpoints/job_silver_ticker_checkpoint_v3\"\n",
    "\n",
    "print(f\"Streaming Strategy:\")\n",
    "print(f\"Checkpoint: {checkpoint_path}\")\n",
    "print(f\"Data (S3): {destination_path}\")\n",
    "\n",
    "# --- 3. WRITE STREAM ---\n",
    "df_result.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .option(\"fs.s3a.access.key\", ACCESS_KEY) \\\n",
    "    .option(\"fs.s3a.secret.key\", SECRET_KEY) \\\n",
    "    .option(\"fs.s3a.session.token\", SESSION_TOKEN) \\\n",
    "    .option(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start(destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d565709d-5f70-4587-bb20-e7b38170c1e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %fs rm -r /Volumes/workspace/default/storage/checkpoints/job_silver_ticker_v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9b118c8-8a6f-46b3-b0a4-0e076cc67a0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %fs rm -r /Volumes/workspace/default/storage/silver/ticker_data_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67055cc7-0d3d-4d38-9420-ab9214d872b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # 1. DELETE EVERYTHING (The \"Double Tap\")\n",
    "# print(\"Deleting Checkpoint...\")\n",
    "# dbutils.fs.rm(\"/Volumes/workspace/default/storage/checkpoints/job_silver_ticker_v2\", True)\n",
    "\n",
    "# print(\"Deleting Silver Table...\")\n",
    "# dbutils.fs.rm(\"/Volumes/workspace/default/storage/silver/ticker_data_v2\", True)\n",
    "\n",
    "# # 2. VERIFY DELETION\n",
    "# # This MUST fail with \"java.io.FileNotFoundException\" or return False\n",
    "# # If it returns True, the deletion failed.\n",
    "# print(f\"Checkpoint Exists? {dbutils.fs.ls('/Volumes/workspace/default/storage/checkpoints/job_silver_ticker')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "585b29d6-a0ee-4b77-baac-38793498112b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType\n",
    "\n",
    "source_path = \"/Volumes/workspace/default/storage/bronze/ticker_data\"\n",
    "\n",
    "print(\"--- 1. READING BRONZE (Batch Mode) ---\")\n",
    "# Read Bronze as a static table (ignores checkpoints)\n",
    "df = spark.read.format(\"delta\").load(source_path)\n",
    "\n",
    "# 1. Define Schema & Parse\n",
    "schema = StructType([\n",
    "    StructField(\"s\", StringType(), True),\n",
    "    StructField(\"currentPrice\", DoubleType(), True),\n",
    "    StructField(\"change\", DoubleType(), True),\n",
    "    StructField(\"changePercent\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", LongType(), True) # Keeping it simple for debug\n",
    "])\n",
    "\n",
    "# Parse JSON\n",
    "df_parsed = df.selectExpr(\"CAST(value AS STRING) as json_val\") \\\n",
    "              .select(F.from_json(\"json_val\", schema).alias(\"data\")) \\\n",
    "              .select(\"data.*\")\n",
    "\n",
    "# Create Timestamp\n",
    "df_parsed = df_parsed.withColumn(\"event_time\", F.col(\"timestamp\").cast(\"timestamp\"))\n",
    "df_parsed = df_parsed.filter(\"changePercent IS NOT NULL AND changePercent != 0\")\n",
    "\n",
    "print(f\"Total Rows after Parsing/Filtering: {df_parsed.count()}\")\n",
    "\n",
    "# --- 2. CHECK SPY vs TICKERS ---\n",
    "df_market = df_parsed.filter(\"s = 'SPY'\") \\\n",
    "    .select(F.col(\"event_time\").alias(\"m_event_time\")) \\\n",
    "    .withColumn(\"key\", F.lit(1))\n",
    "\n",
    "df_tickers = df_parsed.filter(\"s != 'SPY'\") \\\n",
    "    .withColumn(\"key\", F.lit(1))\n",
    "\n",
    "spy_count = df_market.count()\n",
    "ticker_count = df_tickers.count()\n",
    "\n",
    "print(f\"SPY Rows: {spy_count}\")\n",
    "print(f\"Ticker Rows: {ticker_count}\")\n",
    "\n",
    "if spy_count == 0 or ticker_count == 0:\n",
    "    print(\"❌ CRITICAL FAILURE: One side of the join is empty!\")\n",
    "else:\n",
    "    print(\"✅ Both sides have data. Checking Time Alignment...\")\n",
    "\n",
    "    # --- 3. CHECK TIME ALIGNMENT (The most common failure) ---\n",
    "    # We join on the dummy key to see the raw time difference between SPY and Tickers\n",
    "    df_debug_join = df_tickers.join(df_market, \"key\", \"inner\")\n",
    "    \n",
    "    # Calculate difference in seconds\n",
    "    df_debug_join = df_debug_join.withColumn(\n",
    "        \"diff_seconds\", \n",
    "        F.abs(F.col(\"event_time\").cast(\"long\") - F.col(\"m_event_time\").cast(\"long\"))\n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- TIME GAP ANALYSIS ---\")\n",
    "    df_debug_join.select(\"s\", \"event_time\", \"m_event_time\", \"diff_seconds\").show(5, False)\n",
    "    \n",
    "    # Check if ANY rows satisfy your 2-minute (120 seconds) condition\n",
    "    valid_matches = df_debug_join.filter(\"diff_seconds <= 120\").count()\n",
    "    \n",
    "    print(f\"\\n--- CONCLUSION ---\")\n",
    "    print(f\"Total Potential Combinations: {df_debug_join.count()}\")\n",
    "    print(f\"Matches within 2 Minutes (120s): {valid_matches}\")\n",
    "    \n",
    "    if valid_matches == 0:\n",
    "        print(\"❌ FAILURE: Your timestamps are too far apart! Increase the INTERVAL in your join.\")\n",
    "    else:\n",
    "        print(f\"✅ SUCCESS: {valid_matches} rows should pass the join in Streaming.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver Layer stock tickers",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
