{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e44ca294-81a0-49b5-a401-d10ce4a81e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime , timedelta\n",
    "from pyspark.sql.functions import col, lit, input_file_name, current_timestamp,regexp_extract, to_date\n",
    "dbutils.widgets.text(\"start_date\", datetime.now().strftime(\"%Y-%m-%d\"),\"Start date\")\n",
    "dbutils.widgets.text(\"end_date\", datetime.now().strftime(\"%Y-%m-%d\"),\"End date\")\n",
    "\n",
    "dbutils.widgets.text(\"mode\", \"INCREMENTAL\",\"mode\")\n",
    "\n",
    "start_date_str = dbutils.widgets.get(\"start_date\")\n",
    "end_date_str = dbutils.widgets.get(\"end_date\")\n",
    "mode = dbutils.widgets.get(\"mode\")\n",
    "\n",
    "date_format = \"%Y-%m-%d\"\n",
    "\n",
    "start_date = datetime.strptime(start_date_str, date_format).date()\n",
    "end_date = datetime.strptime(end_date_str, date_format).date()\n",
    "\n",
    "if start_date > end_date:\n",
    "    raise ValueError(f\"CRITICAL CONFIG ERROR: Start Date ({start_date}) is after End Date ({end_date}). Please check your parameters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bddad4f8-26e7-4d20-916f-2ad716aa3fad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ACCESS_KEY = dbutils.secrets.get(scope = \"ticker\", key = \"access_key\")\n",
    "SECRET_KEY = dbutils.secrets.get(scope = \"ticker\", key = \"secret_key\")\n",
    "SESSION_TOKEN = dbutils.secrets.get(scope = \"ticker\", key = \"session_key\")\n",
    "\n",
    "temp_ak = dbutils.jobs.taskValues.get(taskKey=\"Init_Auth\", key=\"temp_ak\", debugValue=\"debug-key\")\n",
    "temp_sk = dbutils.jobs.taskValues.get(taskKey=\"Init_Auth\", key=\"temp_sk\", debugValue=\"debug-secret\")\n",
    "temp_token = dbutils.jobs.taskValues.get(taskKey=\"Init_Auth\", key=\"temp_token\", debugValue=\"debug-token\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "288e77e1-c26c-4095-9036-3208e72df8cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Schema Strategy: The \"Wide Net\" Policy\n",
    "We replaced the strict StructType enforcement with a Permissive Read strategy to satisfy the \"Bronze Policy\". Hardcoding the schema at this stage is dangerous because a single unexpected data type (e.g., a string in an integer column) would cause the entire batch to fail or silently drop data. By enabling mode=\"PERMISSIVE\" and capturing malformed records in _rescued_data, we ensure the pipeline never crashes on schema drift, allowing us to debug issues later without losing raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "422c8f78-183a-4e1e-bf35-39dd34094c66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "master_schema = spark.table(\"company_financials_master_def\").schema\n",
    "base_path = \"s3a://mzon-to-databricks-5482/landing/source=fmp/ticker=*\"\n",
    "if mode == \"INCREMENTAL\":\n",
    "    if start_date == end_date:\n",
    "        s3_path = [f\"{base_path}/date={start_date}\"]\n",
    "    else:\n",
    "        delta = (end_date - start_date).days\n",
    "        # FIXED: Loop now includes start_date (i) instead of skipping it (i+1)\n",
    "        date_list = [start_date + timedelta(days=i) for i in range(delta + 1)]\n",
    "        s3_path = [f\"{base_path}/date={d}\" for d in date_list]\n",
    "else:\n",
    "    # Disaster Recovery: Full Reload\n",
    "    s3_path = [f\"{base_path}/**/**\"]\n",
    "print(s3_path)\n",
    "\n",
    "df = (spark.read\n",
    "    \n",
    " \n",
    "       .format(\"text\")\n",
    "       .option(\"wholetext\", \"true\")\n",
    "      .option(\"fs.s3a.access.key\", ACCESS_KEY)\n",
    "      .option(\"fs.s3a.secret.key\", SECRET_KEY)\n",
    "      .option(\"fs.s3a.session.token\", SESSION_TOKEN)\n",
    "      .option(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "      .load(s3_path))\n",
    "\n",
    "\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1da3949-ce2e-4c3c-8da4-062bebddeea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Metadata Extraction: Enabling Partitioning\n",
    "We introduced a transformation step to derive the date column directly from the source file path (using Regex). Since the raw JSON data inside the files does not guarantee a clean date field, relying on the folder structure (.../date=2025-12-21/...) is the only way to accurately map files to their correct partitions. This step is a prerequisite for \"Protocol A,\" as the writer needs this column to know exactly where to place the data in the Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4effac42-5dc7-4bec-baae-d9d9f69d9a79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final = df.withColumn(\"ingestion_timestamp\",current_timestamp()) \\\n",
    "             .withColumn(\"source_file\", col(\"_metadata.file_path\")) \\\n",
    "             .withColumn(\"date\", to_date(regexp_extract(col(\"_metadata.file_path\"), \"date=(\\\\d{4}-\\\\d{2}-\\\\d{2})\", 1), \"yyyy-MM-dd\")) \\\n",
    "             .withColumn(\"symbol\", regexp_extract(col(\"_metadata.file_path\"), \"ticker=([^/]+)\", 1)) \\\n",
    "             .withColumn(\"statement_type\", regexp_extract(col(\"_metadata.file_path\"), \"statement=([^/]+)\", 1))\n",
    "df_final.printSchema()\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57ba57ba-2e5f-48bf-9aa3-44714438952e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write Strategy: Enforcing \"Protocol A\" (Idempotency)\n",
    "We upgraded the write mode from a simple append to the \"Surgical Backfill\" standard. Using append is risky because re-running a job (e.g., after a fix) creates duplicate records. We now use .mode(\"overwrite\") with the replaceWhere condition. This instructs Delta Lake to surgically delete and replace only the data for the specific dates being processed, guaranteeing that we can run the pipeline multiple times safely without creating duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e10f2eba-588e-4712-b667-bb2e7fa7df56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "destination_path = f\"s3a://mzon-to-databricks-5482/bronze/source=fmp/\"\n",
    "\n",
    "write_writer = (df_final.write\n",
    "    .format(\"delta\")\n",
    "    .option(\"mergeSchema\", \"true\") # [cite: 119]\n",
    "   .option(\"fs.s3a.access.key\", ACCESS_KEY)\n",
    "      .option(\"fs.s3a.secret.key\", SECRET_KEY)\n",
    "      .option(\"fs.s3a.session.token\", SESSION_TOKEN)\n",
    "    .option(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    ")\n",
    "\n",
    "# PROTOCOL A: SURGICAL BACKFILL \n",
    "if mode == \"INCREMENTAL\":\n",
    "    # We use replaceWhere to safely overwrite ONLY the days we are re-processing.\n",
    "    # This ensures Idempotency.\n",
    "    condition = f\"date >= '{start_date}' AND date <= '{end_date}'\"\n",
    "    (write_writer\n",
    "     .mode(\"overwrite\")\n",
    "     .option(\"replaceWhere\", condition)\n",
    "     .partitionBy(\"date\", \"symbol\") # Partition by Date is REQUIRED for this to work efficiently\n",
    "     .save(destination_path))\n",
    "else:\n",
    "    # FULL RELOAD [cite: 156]\n",
    "    (write_writer\n",
    "     .mode(\"overwrite\")\n",
    "     .option(\"overwriteSchema\", \"true\")\n",
    "     .partitionBy(\"date\", \"symbol\")\n",
    "     .save(destination_path))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze Layer Fin statements",
   "widgets": {
    "end_date": {
     "currentValue": "2025-12-30",
     "nuid": "16a11d30-6e31-4aee-9b75-77e51a312e3d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2025-12-31",
      "label": "End date",
      "name": "end_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2025-12-31",
      "label": "End date",
      "name": "end_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "mode": {
     "currentValue": "full",
     "nuid": "aff1fc57-0a74-4687-924e-b6ad9e6281a7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "INCREMENTAL",
      "label": "mode",
      "name": "mode",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "INCREMENTAL",
      "label": "mode",
      "name": "mode",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "start_date": {
     "currentValue": "2025-12-23",
     "nuid": "315d84ae-62a1-4723-a5a7-f17d0f2e44d2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2025-12-31",
      "label": "Start date",
      "name": "start_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2025-12-31",
      "label": "Start date",
      "name": "start_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
