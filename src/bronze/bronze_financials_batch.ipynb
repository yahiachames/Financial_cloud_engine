{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e44ca294-81a0-49b5-a401-d10ce4a81e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime , timedelta\n",
    "from pyspark.sql.functions import col, lit, input_file_name, current_timestamp,regexp_extract, to_date\n",
    "dbutils.widgets.text(\"start_date\", datetime.now().strftime(\"%Y-%m-%d\"),\"Start date\")\n",
    "dbutils.widgets.text(\"end_date\", datetime.now().strftime(\"%Y-%m-%d\"),\"End date\")\n",
    "\n",
    "dbutils.widgets.text(\"mode\", \"INCREMENTAL\",\"mode\")\n",
    "\n",
    "start_date_str = dbutils.widgets.get(\"start_date\")\n",
    "end_date_str = dbutils.widgets.get(\"end_date\")\n",
    "mode = dbutils.widgets.get(\"mode\")\n",
    "\n",
    "date_format = \"%Y-%m-%d\"\n",
    "\n",
    "start_date = datetime.strptime(start_date_str, date_format).date()\n",
    "end_date = datetime.strptime(end_date_str, date_format).date()\n",
    "\n",
    "if start_date > end_date:\n",
    "    raise ValueError(f\"CRITICAL CONFIG ERROR: Start Date ({start_date}) is after End Date ({end_date}). Please check your parameters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bddad4f8-26e7-4d20-916f-2ad716aa3fad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ACCESS_KEY = dbutils.secrets.get(scope = \"ticker\", key = \"access_key\")\n",
    "SECRET_KEY = dbutils.secrets.get(scope = \"ticker\", key = \"secret_key\")\n",
    "SESSION_TOKEN = dbutils.secrets.get(scope = \"ticker\", key = \"session_key\")\n",
    "\n",
    "temp_ak = dbutils.jobs.taskValues.get(taskKey=\"Init_Auth\", key=\"temp_ak\", debugValue=\"debug-key\")\n",
    "temp_sk = dbutils.jobs.taskValues.get(taskKey=\"Init_Auth\", key=\"temp_sk\", debugValue=\"debug-secret\")\n",
    "temp_token = dbutils.jobs.taskValues.get(taskKey=\"Init_Auth\", key=\"temp_token\", debugValue=\"debug-token\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "288e77e1-c26c-4095-9036-3208e72df8cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Schema Evolution Strategy: Permissive Read (The \"Wide Net\" Policy)\n",
    "To comply with Bronze Layer requirements, we replace strict strict `StructType` enforcement with a **Permissive Read** strategy. \n",
    "\n",
    "**Rationale**:\n",
    "- Hardcoding the schema at this stage is risky; a single unexpected data type (e.g., a string in an integer column) would cause the entire batch to fail.\n",
    "- By enabling `mode=\"PERMISSIVE\"` and capturing malformed records in `_rescued_data`, we ensure the pipeline is resilient to schema drift. This allows for upstream debugging without data loss in the raw layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "422c8f78-183a-4e1e-bf35-39dd34094c66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "master_schema = spark.table(\"company_financials_master_def\").schema\n",
    "base_path = \"s3a://mzon-to-databricks-5482/landing/source=fmp/ticker=*\"\n",
    "table_root = \"s3a://mzon-to-databricks-5482/landing/source=fmp/\"\n",
    "if mode == \"INCREMENTAL\":\n",
    "    if start_date == end_date:\n",
    "        s3_path = [f\"{base_path}/date={start_date}\"]\n",
    "    else:\n",
    "        delta = (end_date - start_date).days\n",
    "        # FIXED: Loop now includes start_date (i) instead of skipping it (i+1)\n",
    "        date_list = [start_date + timedelta(days=i) for i in range(delta + 1)]\n",
    "        s3_path = [f\"{base_path}/date={d}\" for d in date_list]\n",
    "else:\n",
    "    # Disaster Recovery: Full Reload\n",
    "    s3_path = [f\"{base_path}/**/**\"]\n",
    "print(s3_path)\n",
    "\n",
    "df = (spark.read\n",
    "    \n",
    " \n",
    "       .format(\"text\")\n",
    "       .option(\"wholetext\", \"true\")\n",
    "     .option(\"fs.s3a.access.key\", temp_ak)\n",
    "      .option(\"fs.s3a.secret.key\", temp_sk)\n",
    "      .option(\"fs.s3a.session.token\", temp_token)\n",
    "      .option(\"basePath\", table_root)\n",
    "      .option(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "      .load(s3_path))\n",
    "\n",
    "\n",
    "\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1da3949-ce2e-4c3c-8da4-062bebddeea6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Metadata Extraction: Partitioning Key Derivation\n",
    "\n",
    "We extract the date column directly from the source file path using Regex. Since the raw JSON content may not guarantee a standard date field, relying on the folder structure (`.../date=2025-12-21/...`) provides a reliable partition key. This extraction is a prerequisite for the **Idempotent Backfill**, as the writer requires this column to accurately target partitions for replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4effac42-5dc7-4bec-baae-d9d9f69d9a79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final = df.withColumn(\"ingestion_timestamp\",current_timestamp()) \\\n",
    "             .withColumn(\"source_file\", col(\"_metadata.file_path\")) \\\n",
    "             .withColumn(\"date\", to_date(regexp_extract(col(\"_metadata.file_path\"), \"date=(\\\\d{4}-\\\\d{2}-\\\\d{2})\", 1), \"yyyy-MM-dd\")) \\\n",
    "             .withColumn(\"symbol\", regexp_extract(col(\"_metadata.file_path\"), \"ticker=([^/]+)\", 1)) \\\n",
    "             .withColumn(\"statement_type\", regexp_extract(col(\"_metadata.file_path\"), \"statement=([^/]+)\", 1))\n",
    "df_final.printSchema()\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57ba57ba-2e5f-48bf-9aa3-44714438952e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write Strategy: Idempotent Backfill\n",
    "\n",
    "We enforce **Idempotency** using the `replaceWhere` option. \n",
    "\n",
    "- **Standard Append**: Risky, as re-running a job creates duplicate records.\n",
    "- **Surgical Backfill**: By using `.mode(\"overwrite\")` with a specific date range condition, Delta Lake surgically deletes and replaces only the data for the processed dates. This guarantees safe re-runs without data duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e10f2eba-588e-4712-b667-bb2e7fa7df56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "destination_path = f\"s3a://mzon-to-databricks-5482/bronze/source=fmp/\"\n",
    "\n",
    "write_writer = (df_final.write\n",
    "    .format(\"delta\")\n",
    "    .option(\"mergeSchema\", \"true\") # [cite: 119]\n",
    "   .option(\"fs.s3a.access.key\", temp_ak)\n",
    "      .option(\"fs.s3a.secret.key\", temp_sk)\n",
    "      .option(\"fs.s3a.session.token\", temp_token)\n",
    "    .option(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    ")\n",
    "\n",
    "# PROTOCOL A: SURGICAL BACKFILL \n",
    "if mode == \"INCREMENTAL\":\n",
    "    # We use replaceWhere to safely overwrite ONLY the days we are re-processing.\n",
    "    # This ensures Idempotency.\n",
    "    condition = f\"date >= '{start_date}' AND date <= '{end_date}'\"\n",
    "    (write_writer\n",
    "     .mode(\"overwrite\")\n",
    "     .option(\"replaceWhere\", condition)\n",
    "     .partitionBy(\"date\", \"symbol\") # Partition by Date is REQUIRED for this to work efficiently\n",
    "     .save(destination_path))\n",
    "else:\n",
    "    # FULL RELOAD [cite: 156]\n",
    "    (write_writer\n",
    "     .mode(\"overwrite\")\n",
    "     .option(\"overwriteSchema\", \"true\")\n",
    "     .partitionBy(\"date\", \"symbol\")\n",
    "     .save(destination_path))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_financials_batch.ipynb",
   "widgets": {
    "end_date": {
     "currentValue": "2025-12-30",
     "nuid": "16a11d30-6e31-4aee-9b75-77e51a312e3d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2026-01-10",
      "label": "End date",
      "name": "end_date",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "2026-01-10",
      "label": "End date",
      "name": "end_date",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "mode": {
     "currentValue": "full",
     "nuid": "aff1fc57-0a74-4687-924e-b6ad9e6281a7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "INCREMENTAL",
      "label": "mode",
      "name": "mode",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "INCREMENTAL",
      "label": "mode",
      "name": "mode",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "start_date": {
     "currentValue": "2025-12-23",
     "nuid": "315d84ae-62a1-4723-a5a7-f17d0f2e44d2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2026-01-10",
      "label": "Start date",
      "name": "start_date",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "2026-01-10",
      "label": "Start date",
      "name": "start_date",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
