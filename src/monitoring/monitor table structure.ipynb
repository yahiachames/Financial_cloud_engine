{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d4245bb-1937-485b-b1ed-4e1c77da5eb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# The Data Optimization Dilemma\n",
    "\n",
    "## 1. What is the Issue?\n",
    "We are currently trapped in a cycle of **Reactive Maintenance**.\n",
    "* **The Bottleneck:** As data grows, query performance degrades (SLA breaches).\n",
    "* **The Burden:** Engineers must manually intervene to analyze skew, identify slow queries, and re-engineer table structures.\n",
    "* **The Risk:** Traditional fixes like **Repartitioning** require full table rewrites (`overwriteSchema`), which are expensive, risky, and lock the table for long periods.\n",
    "* **The Complexity:** We are forced to maintain complex \"Auto-Tuner\" scripts that rely on fragile regex parsing of query history just to guess the right configuration.\n",
    "\n",
    "## 2. What We Hope We Could Do\n",
    "Our ideal state is an **Autonomous Data Platform**:\n",
    "* **Self-Optimizing:** The system should automatically \"learn\" which columns are frequently accessed without us writing regex parsers.\n",
    "* **Flexible:** We should be able to change how data is clustered (e.g., from `date` to `region`) instantly, without rewriting 10 years of history.\n",
    "* **Resilient:** The system should handle data skew automatically, preventing \"straggler\" tasks without manual \"salting\" techniques.\n",
    "* **Zero-Touch:** We want to define *what* we want (fast queries on `symbol`), not *how* to store it (file layout).\n",
    "\n",
    "## 3. What We Can Do Today (The \"Hard Way\")\n",
    "Currently, to achieve performance, we are forced to build complex \"Rube Goldberg\" machines:\n",
    "1.  **Query Mining:** We scrape `system.query.history` to find frequent columns.\n",
    "2.  **Rigid Partitioning:** We hard-code a physical directory structure (e.g., `/date=2024-01-01/`). If access patterns change, this structure becomes a prison.\n",
    "3.  **Scheduled Heavy Lifting:** We run expensive `OPTIMIZE ZORDER BY` jobs that fight for resources.\n",
    "4.  **Manual Skew Fixes:** When a partition gets too big, we manually intervene to split it.\n",
    "\n",
    "## 4. How We Will Get to Our Hope: Liquid Clustering\n",
    "We bridge this gap by abandoning physical directory partitioning entirely and adopting **Delta Liquid Clustering**.\n",
    "\n",
    "* **The Shift:** Instead of rigid physical folders, Liquid Clustering uses a flexible, dynamic index managed by Delta Lake.\n",
    "* **The Solution:**\n",
    "    * **No Rewrites:** We can redefine clustering keys on the fly using `ALTER TABLE`.\n",
    "    * **Skew Handling:** It automatically detects skew and splits data into appropriate sizes without user intervention.\n",
    "    * **Simplicity:** It replaces our entire 100-line \"AutoTuner\" script with a single line of configuration.\n",
    "\n",
    "> **Next Step:** We will demonstrate how replacing `PARTITIONED BY` with `CLUSTER BY` eliminates the need for the complex Python logic we just wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a672b0e-d4d7-4371-86f5-43425f3f3a07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, lower, explode, split, expr\n",
    "import re\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "TARGET_TABLE = \"silver_audit\"\n",
    "\n",
    "class TableAutoTuner:\n",
    "    def __init__(self, spark, table_name):\n",
    "        self.spark = spark\n",
    "        self.table_name = table_name\n",
    "        self.valid_columns = []\n",
    "        self.candidates = []\n",
    "\n",
    "    def _get_valid_columns(self):\n",
    "        \"\"\"\n",
    "        Fetches the actual schema of the table to validate extracted text.\n",
    "        \"\"\"\n",
    "        print(f\"[INFO] Fetching schema for {self.table_name}...\")\n",
    "        try:\n",
    "            # Get list of actual column names (e.g., ['symbol', 'date', 'eps'])\n",
    "            self.valid_columns = self.spark.read.table(self.table_name).columns\n",
    "            print(f\"[INFO] Valid Schema Columns: {self.valid_columns}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Could not read table schema: {str(e)}\")\n",
    "\n",
    "    def _extract_columns_from_history(self):\n",
    "        \"\"\"\n",
    "        Parses system history and finds which valid columns are used most often.\n",
    "        \"\"\"\n",
    "        print(f\"[INFO] Mining query history for {self.table_name}...\")\n",
    "        \n",
    "        # 1. Fetch raw SQL statements involving this table\n",
    "        history_df = self.spark.sql(f\"\"\"\n",
    "            SELECT statement_text \n",
    "            FROM system.query.history \n",
    "            WHERE statement_text LIKE '%{self.table_name}%'\n",
    "              AND statement_type = 'SELECT'\n",
    "              AND start_time > date_add(now(), -30)\n",
    "        \"\"\")\n",
    "\n",
    "        if history_df.count() == 0:\n",
    "            print(\"[WARN] No history found. Cannot optimize.\")\n",
    "            return\n",
    "\n",
    "        # 2. Tokenize the SQL text\n",
    "        # We split by non-alphanumeric characters to isolate words\n",
    "        words_df = (history_df\n",
    "                    .select(explode(split(lower(col(\"statement_text\")), \"[^a-z0-9_]\")).alias(\"word\"))\n",
    "                    .filter(col(\"word\") != \"\")\n",
    "                   )\n",
    "\n",
    "        # 3. Filter for words that are ACTUAL table columns\n",
    "        # This prevents \"WHERE\" or \"SELECT\" from being treated as column names\n",
    "        # AND prevents errors like 'company_symbol' vs 'symbol'\n",
    "        valid_cols_broadcast = [c.lower() for c in self.valid_columns]\n",
    "        \n",
    "        matched_cols_df = (words_df\n",
    "                           .filter(col(\"word\").isin(valid_cols_broadcast))\n",
    "                           .groupBy(\"word\")\n",
    "                           .agg(count(\"*\").alias(\"frequency\"))\n",
    "                           .sort(col(\"frequency\").desc())\n",
    "                           .limit(5) # Top 5 used columns\n",
    "                          )\n",
    "\n",
    "        print(\"[INFO] Top detected columns from history:\")\n",
    "        matched_cols_df.show()\n",
    "        \n",
    "        # Convert to list for further processing\n",
    "        self.candidates = [row[\"word\"] for row in matched_cols_df.collect()]\n",
    "\n",
    "    def _analyze_cardinality_and_recommend(self):\n",
    "        \"\"\"\n",
    "        Decides between Partitioning vs. Z-Order based on unique values.\n",
    "        \"\"\"\n",
    "        if not self.candidates:\n",
    "            print(\"[INFO] No valid usage patterns found.\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n--- Optimization Recommendations ---\")\n",
    "        \n",
    "        for col_name in self.candidates:\n",
    "            try:\n",
    "                # Check how many unique values exist\n",
    "                distinct_count = self.spark.table(self.table_name).select(col_name).distinct().count()\n",
    "                \n",
    "                print(f\"Column: '{col_name}' | Distinct Values: {distinct_count}\")\n",
    "                \n",
    "                # LOGIC MATRIX\n",
    "                if distinct_count > 10000:\n",
    "                    print(f\"  -> RECOMMENDATION: **Z-ORDER** (High Cardinality).\")\n",
    "                    print(f\"     Why: Too many unique values for partitioning. Z-Order will skip data effectively.\")\n",
    "                    \n",
    "                elif 50 < distinct_count <= 10000:\n",
    "                    print(f\"  -> RECOMMENDATION: **Z-ORDER** (Medium Cardinality).\")\n",
    "                    print(f\"     Why: Safe for Z-Order. Risky for partitioning (potential small files).\")\n",
    "                    \n",
    "                else: # < 50\n",
    "                    print(f\"  -> RECOMMENDATION: **PARTITION** (Low Cardinality).\")\n",
    "                    print(f\"     Why: Few unique values. Good candidate for physical folder separation.\")\n",
    "                    \n",
    "                print(\"-\" * 50)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] failed to analyze {col_name}: {str(e)}\")\n",
    "\n",
    "    def run(self):\n",
    "        print(f\"--- Auto-Discovery Tuner for {self.table_name} ---\")\n",
    "        self._get_valid_columns()\n",
    "        self._extract_columns_from_history()\n",
    "        self._analyze_cardinality_and_recommend()\n",
    "\n",
    "# --- EXECUTION ---\n",
    "tuner = TableAutoTuner(spark, TARGET_TABLE)\n",
    "tuner.run()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "monitor table structure",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
