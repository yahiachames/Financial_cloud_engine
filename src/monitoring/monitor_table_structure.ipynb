{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {},
                    "inputWidgets": {},
                    "nuid": "5d4245bb-1937-485b-b1ed-4e1c77da5eb0",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "# Autonomous Performance Tuning: Partitioning vs. Clustering\n",
                "\n",
                "## 1. The Challenge: Static Partitioning Limitations\n",
                "Traditional Hive-style partitioning (e.g., partitioning by `date`) can become a bottleneck as data volume and access patterns evolve.\n",
                "*   **Data Skew:** Daily partitions may vary significantly in size, leading to inefficient resource utilization.\n",
                "*   **Maintenance Overhead:** Changing partition keys requires costly `full overwrite` operations.\n",
                "*   **Complexity:** Optimizing for multiple access patterns often involves complex, manual Z-Ordering routines.\n",
                "\n",
                "## 2. The Solution: Delta Liquid Clustering\n",
                "We are transitioning towards **Delta Liquid Clustering** to enable autonomous, self-managing data layouts.\n",
                "*   **Self-Managing:** Delta Lake automatically clusters data based on access patterns without rigid physical directories.\n",
                "*   **Flexible:** Clustering keys can be redefined instantly via `ALTER TABLE` without rewriting history.\n",
                "*   **Skew Resilient:** Automatically handles unequal data distribution.\n",
                "\n",
                "## 3. Auto-Discovery Logic\n",
                "This script serves as a \"Tuning Advisor.\" It:\n",
                "1.  **Analyzes Query History:** Scrapes `system.query.history` to identify frequently accessed columns.\n",
                "2.  **Evaluates Cardinality:** Checks column statistics to determine the optimal clustering strategy.\n",
                "3.  **Recommends Configuration:** Suggests `CLUSTER BY` vs `PARTITIONED BY` based on data characteristics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "5a672b0e-d4d7-4371-86f5-43425f3f3a07",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import col, count, lower, explode, split, expr\n",
                "import re\n",
                "\n",
                "# --- CONFIGURATION ---\n",
                "TARGET_TABLE = \"silver_audit\"\n",
                "\n",
                "class TableAutoTuner:\n",
                "    def __init__(self, spark, table_name):\n",
                "        self.spark = spark\n",
                "        self.table_name = table_name\n",
                "        self.valid_columns = []\n",
                "        self.candidates = []\n",
                "\n",
                "    def _get_valid_columns(self):\n",
                "        \"\"\"\n",
                "        Fetches the actual schema of the table to validate extracted text.\n",
                "        \"\"\"\n",
                "        print(f\"[INFO] Fetching schema for {self.table_name}...\")\n",
                "        try:\n",
                "            # Get list of actual column names (e.g., ['symbol', 'date', 'eps'])\n",
                "            self.valid_columns = self.spark.read.table(self.table_name).columns\n",
                "            print(f\"[INFO] Valid Schema Columns: {self.valid_columns}\")\n",
                "        except Exception as e:\n",
                "            print(f\"[ERROR] Could not read table schema: {str(e)}\")\n",
                "\n",
                "    def _extract_columns_from_history(self):\n",
                "        \"\"\"\n",
                "        Parses system history and finds which valid columns are used most often.\n",
                "        \"\"\"\n",
                "        print(f\"[INFO] Mining query history for {self.table_name}...\")\n",
                "        \n",
                "        # 1. Fetch raw SQL statements involving this table\n",
                "        history_df = self.spark.sql(f\"\"\"\n",
                "            SELECT statement_text \n",
                "            FROM system.query.history \n",
                "            WHERE statement_text LIKE '%{self.table_name}%'\n",
                "              AND statement_type = 'SELECT'\n",
                "              AND start_time > date_add(now(), -30)\n",
                "        \"\"\")\n",
                "\n",
                "        if history_df.count() == 0:\n",
                "            print(\"[WARN] No history found. Cannot optimize.\")\n",
                "            return\n",
                "\n",
                "        # 2. Tokenize the SQL text\n",
                "        # We split by non-alphanumeric characters to isolate words\n",
                "        words_df = (history_df\n",
                "                    .select(explode(split(lower(col(\"statement_text\")), \"[^a-z0-9_]\")).alias(\"word\"))\n",
                "                    .filter(col(\"word\") != \"\")\n",
                "                   )\n",
                "\n",
                "        # 3. Filter for words that are ACTUAL table columns\n",
                "        # This prevents \"WHERE\" or \"SELECT\" from being treated as column names\n",
                "        # AND prevents errors like 'company_symbol' vs 'symbol'\n",
                "        valid_cols_broadcast = [c.lower() for c in self.valid_columns]\n",
                "        \n",
                "        matched_cols_df = (words_df\n",
                "                           .filter(col(\"word\").isin(valid_cols_broadcast))\n",
                "                           .groupBy(\"word\")\n",
                "                           .agg(count(\"*\").alias(\"frequency\"))\n",
                "                           .sort(col(\"frequency\").desc())\n",
                "                           .limit(5) # Top 5 used columns\n",
                "                          )\n",
                "\n",
                "        print(\"[INFO] Top detected columns from history:\")\n",
                "        matched_cols_df.show()\n",
                "        \n",
                "        # Convert to list for further processing\n",
                "        self.candidates = [row[\"word\"] for row in matched_cols_df.collect()]\n",
                "\n",
                "    def _analyze_cardinality_and_recommend(self):\n",
                "        \"\"\"\n",
                "        Decides between Partitioning vs. Z-Order based on unique values.\n",
                "        \"\"\"\n",
                "        if not self.candidates:\n",
                "            print(\"[INFO] No valid usage patterns found.\")\n",
                "            return\n",
                "\n",
                "        print(\"\\n--- Optimization Recommendations ---\")\n",
                "        \n",
                "        for col_name in self.candidates:\n",
                "            try:\n",
                "                # Check how many unique values exist\n",
                "                distinct_count = self.spark.table(self.table_name).select(col_name).distinct().count()\n",
                "                \n",
                "                print(f\"Column: '{col_name}' | Distinct Values: {distinct_count}\")\n",
                "                \n",
                "                # LOGIC MATRIX\n",
                "                if distinct_count > 10000:\n",
                "                    print(f\"  -> RECOMMENDATION: **Z-ORDER** (High Cardinality).\")\n",
                "                    print(f\"     Why: Too many unique values for partitioning. Z-Order will skip data effectively.\")\n",
                "                    \n",
                "                elif 50 < distinct_count <= 10000:\n",
                "                    print(f\"  -> RECOMMENDATION: **Z-ORDER** (Medium Cardinality).\")\n",
                "                    print(f\"     Why: Safe for Z-Order. Risky for partitioning (potential small files).\")\n",
                "                    \n",
                "                else: # < 50\n",
                "                    print(f\"  -> RECOMMENDATION: **PARTITION** (Low Cardinality).\")\n",
                "                    print(f\"     Why: Few unique values. Good candidate for physical folder separation.\")\n",
                "                    \n",
                "                print(\"-\" * 50)\n",
                "\n",
                "            except Exception as e:\n",
                "                print(f\"[ERROR] failed to analyze {col_name}: {str(e)}\")\n",
                "\n",
                "    def run(self):\n",
                "        print(f\"--- Auto-Discovery Tuner for {self.table_name} ---\")\n",
                "        self._get_valid_columns()\n",
                "        self._extract_columns_from_history()\n",
                "        self._analyze_cardinality_and_recommend()\n",
                "\n",
                "# --- EXECUTION ---\n",
                "tuner = TableAutoTuner(spark, TARGET_TABLE)\n",
                "tuner.run()"
            ]
        }
    ],
    "metadata": {
        "application/vnd.databricks.v1+notebook": {
            "computePreferences": null,
            "dashboards": [],
            "environmentMetadata": {
                "base_environment": "",
                "environment_version": "4"
            },
            "inputWidgetPreferences": null,
            "language": "python",
            "notebookMetadata": {
                "pythonIndentUnit": 4
            },
            "notebookName": "monitor_table_structure",
            "widgets": {}
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}