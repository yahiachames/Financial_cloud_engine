{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "6b432506-0f31-4e98-9abe-a4b272486e14",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "source": [
                "# Synchronization Pipeline: S3 to Databricks Volumes\n",
                "\n",
                "## Objective\n",
                "This pipeline mirrors the \"Golden\" S3 data into Databricks Volumes (`/Volumes/...`).\n",
                "\n",
                "## Architectural Benefit\n",
                "By syncing data to Unity Catalog Volumes, we decouple interactive querying from AWS Key management. Users and Dashboards can query the Volume-backed SQL Views without requiring injected session tokens, enabling a seamless \"Serverless-like\" experience."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "91a87754-5264-42b4-8254-8c85743202e8",
                    "showTitle": false,
                    "tableResultSettingsMap": {},
                    "title": ""
                }
            },
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import col\n",
                "from pyspark.sql import SparkSession\n",
                "\n",
                "# --- 1. CONFIGURATION & AUTH ---\n",
                "# We need keys ONE LAST TIME to read the source S3 data.\n",
                "\n",
                "ACCESS_KEY = dbutils.secrets.get(scope = \"ticker\", key = \"access_key\")\n",
                "SECRET_KEY = dbutils.secrets.get(scope = \"ticker\", key = \"secret_key\")\n",
                "SESSION_TOKEN = dbutils.secrets.get(scope = \"ticker\", key = \"session_key\")\n",
                "\n",
                "# Or use TaskValues if running in a job chain\n",
                "try:\n",
                "    temp_ak = dbutils.jobs.taskValues.get(taskKey=\"Init_Auth\", key=\"temp_ak\")\n",
                "    temp_sk = dbutils.jobs.taskValues.get(taskKey=\"Init_Auth\", key=\"temp_sk\")\n",
                "    temp_token = dbutils.jobs.taskValues.get(taskKey=\"Init_Auth\", key=\"temp_token\")\n",
                "except:\n",
                "    # Fallback for manual run\n",
                "    temp_ak, temp_sk, temp_token = ACCESS_KEY, SECRET_KEY, SESSION_TOKEN\n",
                "\n",
                "# --- 2. DEFINITIONS ---\n",
                "\n",
                "# ROOT PATHS (Extracted from your code)\n",
                "S3_BASE_SILVER = \"s3a://mzon-to-databricks-5482/silver/valid\"\n",
                "S3_BASE_GOLD   = \"s3a://mzon-to-databricks-5482/gold/valid\"\n",
                "\n",
                "# VOLUME PATHS (Target Destination)\n",
                "VOL_BASE = \"/Volumes/workspace/default/storage\"\n",
                "\n",
                "# MAPPING: Source S3 -> Target Volume -> View Name\n",
                "sync_map = [\n",
                "    {\n",
                "        \"name\": \"Silver Income Statement\",\n",
                "        \"s3_path\": f\"{S3_BASE_SILVER}/income_statement\",\n",
                "        \"vol_path\": f\"{VOL_BASE}/silver/income_statement\",\n",
                "        \"view_name\": \"silver_income_statement\"\n",
                "    },\n",
                "    {\n",
                "        \"name\": \"Silver Balance Sheet\",\n",
                "        \"s3_path\": f\"{S3_BASE_SILVER}/balance_sheet\",\n",
                "        \"vol_path\": f\"{VOL_BASE}/silver/balance_sheet\",\n",
                "        \"view_name\": \"silver_balance_sheet\"\n",
                "    },\n",
                "    {\n",
                "        \"name\": \"Silver Cash Flow\",\n",
                "        \"s3_path\": f\"{S3_BASE_SILVER}/cashflow_statement\",\n",
                "        \"vol_path\": f\"{VOL_BASE}/silver/cashflow_statement\",\n",
                "        \"view_name\": \"silver_cashflow_statement\"\n",
                "    },\n",
                "    {\n",
                "        \"name\": \"Gold Financial Ratios\",\n",
                "        \"s3_path\": f\"{S3_BASE_GOLD}\", # Gold path was direct, not nested\n",
                "        \"vol_path\": f\"{VOL_BASE}/gold/financial_ratios_batch\",\n",
                "        \"view_name\": \"gold_financial_ratios\"\n",
                "    }\n",
                "]\n",
                "\n",
                "# --- 3. EXECUTION ENGINE ---\n",
                "\n",
                "def sync_and_register(meta):\n",
                "    print(f\"--- Processing: {meta['name']} ---\")\n",
                "    \n",
                "    # A. READ from S3 (With Keys)\n",
                "    try:\n",
                "        df_source = (spark.read\n",
                "            .format(\"delta\")\n",
                "            .option(\"fs.s3a.access.key\", temp_ak)\n",
                "            .option(\"fs.s3a.secret.key\", temp_sk)\n",
                "            .option(\"fs.s3a.session.token\", temp_token)\n",
                "            .option(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
                "            .load(meta['s3_path'])\n",
                "        )\n",
                "        print(f\"1. Read S3 Success. Rows: {df_source.count()}\")\n",
                "    except Exception as e:\n",
                "        print(f\"SKIP: Could not read S3 path {meta['s3_path']}. Error: {e}\")\n",
                "        return\n",
                "\n",
                "    # B. WRITE to Volume (No Keys Needed)\n",
                "    # We use 'overwrite' to ensure the Volume is an exact mirror of S3\n",
                "    (df_source.write\n",
                "        .format(\"delta\")\n",
                "        .mode(\"overwrite\")\n",
                "        .option(\"overwriteSchema\", \"true\") \n",
                "        .save(meta['vol_path'])\n",
                "    )\n",
                "    print(f\"2. Wrote to Volume: {meta['vol_path']}\")\n",
                "\n",
                "    # C. CREATE VIEW (The Pointer)\n",
                "    # We use backticks ` ` for the volume path\n",
                "    spark.sql(f\"\"\"\n",
                "        CREATE OR REPLACE VIEW {meta['view_name']} \n",
                "        AS SELECT * FROM delta.`{meta['vol_path']}`\n",
                "    \"\"\")\n",
                "    print(f\"3. Registered View: {meta['view_name']}\")\n",
                "    \n",
                "    # D. OPTIMIZE (Performance)\n",
                "    spark.sql(f\"OPTIMIZE delta.`{meta['vol_path']}`\")\n",
                "    print(\"4. Optimization Complete\")\n",
                "    print(\"-\" * 30)\n",
                "\n",
                "# --- 4. RUN ---\n",
                "\n",
                "for item in sync_map:\n",
                "    sync_and_register(item)\n",
                "\n",
                "print(\"\\nSUCCESS: All S3 tables synced to Volume and Views created.\")"
            ]
        }
    ],
    "metadata": {
        "application/vnd.databricks.v1+notebook": {
            "computePreferences": null,
            "dashboards": [],
            "environmentMetadata": {
                "base_environment": "",
                "environment_version": "4"
            },
            "inputWidgetPreferences": null,
            "language": "python",
            "notebookMetadata": {
                "mostRecentlyExecutedCommandWithImplicitDF": {
                    "commandId": 6200691363269828,
                    "dataframes": [
                        "_sqldf"
                    ]
                },
                "pythonIndentUnit": 4
            },
            "notebookName": "s3_to_volumes",
            "widgets": {}
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}