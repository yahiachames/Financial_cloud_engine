{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a935d41a-97ef-43da-a93a-944bc310efd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "create databricks secret : \n",
    "databricks secrets create-scope ticker\n",
    "databricks secrets list-scopes\n",
    "databricks secrets delete-scope ticker\n",
    "Create a secret\n",
    "\n",
    "databricks secrets put-secret --json '{\n",
    "  \"scope\": \"ticker\",\n",
    "  \"key\": \"financialmodelprep_token\",\n",
    "  \"string_value\": \"apikey\"\n",
    "}'\n",
    "\n",
    "\n",
    "databricks secrets put-secret --json '{\n",
    "  \"scope\": \"ticker\",\n",
    "  \"key\": \"access_key\",\n",
    "  \"string_value\": \"access_key\"\n",
    "}'\n",
    "\n",
    "\n",
    "databricks secrets put-secret --json '{\n",
    "  \"scope\": \"ticker\",\n",
    "  \"key\": \"secret_key\",\n",
    "  \"string_value\": \"secret_key\"\n",
    "}'\n",
    "databricks secrets put-secret --json '{\n",
    "  \"scope\": \"ticker\",\n",
    "  \"key\": \"session_key\",\n",
    "  \"string_value\": \"session_key\"\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d560cd23-b907-43dd-aaaa-87e8ea75d920",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "dbutils.widgets.text(\"run_date\", datetime.now().strftime(\"%Y-%m-%d\"),\"Run date\")\n",
    "run_date = dbutils.widgets.get(\"run_date\")\n",
    "\n",
    "APIKey = dbutils.secrets.get(scope = \"ticker\", key = \"financialmodelprep_token\")\n",
    "BaseUrl = \"https://financialmodelingprep.com/stable/income-statement\"\n",
    "TICKERS = [\"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"NVDA\", \"META\", \"JPM\", \"V\", \"JNJ\",\"PG\"]\n",
    "# TICKERS = [\"AAPL\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf1f8ea0-2eba-4a3f-9b85-c46c7dd90b54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_ticker(ticker,run_date):\n",
    "    Query = f\"{BaseUrl}?symbol={ticker}&limit=5&apikey={APIKey}\"\n",
    "    print(Query)\n",
    "    filename = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    try:\n",
    "        response = requests.get(Query, timeout=10)\n",
    "        response.raise_for_status() # Raises an HTTPError for bad responses (4xx or 5xx) \n",
    "        data = response.json()\n",
    "\n",
    "        # CRITICAL Partitioning for Spark: /source=fmp/ticker=AAPL/date=2023-10-25/\n",
    "        file_name = f\"landing/source=fmp/ticker={ticker}/date={run_date}/{filename}.json\"\n",
    "        \n",
    "        json_data = json.dumps(data, indent=2)\n",
    "        return {\"file_name\": file_name, \"json_data\": json_data,\"status\" : response.status_code}\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing ticker {ticker}: {e}\")\n",
    "        file_name = f\"landing/source=fmp/audit_failures/ticker={ticker}/date={run_date}/{filename}.json\"\n",
    "        status_code =  response.status_code if 'response' in locals() else 500\n",
    "        error_log = {\n",
    "            \"error\": str(e),\n",
    "            \"ticker\": ticker,\n",
    "            \"status_code\": status_code\n",
    "        }\n",
    "        return {\"file_name\": file_name, \"json_data\": json.dumps(error_log,indent=2), \"status\" : status_code}\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0cde2c6-ac3b-45db-8132-8374bd03cdb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tickers_data = [process_ticker(ticker,run_date) for ticker in TICKERS]\n",
    "print(f\"data length {len(tickers_data)} and symbols found {[ticker['file_name'] for ticker in tickers_data]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea6409e8-6d1c-41df-8cbd-6e004d540ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To connect and store data fro mdatabricks to s3\n",
    "we need IAM role in AWS and storage crednetial in databricks created in unity catalog\n",
    "i have no access to create a role or group of policies in AWS that said  i will use less secure method and coomunciate directy with s3\n",
    "unfortunally as i am using aws lab educate , i can do nothing iwth i am i will teh lab secrects and key to communciate with s3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dafcf42-ba83-4c8c-8ff8-6341647c024c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "ACCESS_KEY = dbutils.secrets.get(scope = \"ticker\", key = \"access_key\")\n",
    "SECRET_KEY = dbutils.secrets.get(scope = \"ticker\", key = \"secret_key\")\n",
    "SESSION_TOKEN = dbutils.secrets.get(scope = \"ticker\", key = \"session_key\")\n",
    "\n",
    "# 3. Initialize S3 client or resource\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY,\n",
    "    aws_session_token=SESSION_TOKEN\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for ticker in tickers_data:\n",
    "    print(ticker['file_name'])\n",
    "\n",
    "    # 4. Specify your bucket name and the desired file name (key) in S3\n",
    "    bucket_name = 'arn:aws:s3:us-east-1:180250667274:accesspoint/accesspoint-to-datab'\n",
    "    s3_object_key = ticker['file_name']\n",
    "\n",
    "    # 5. Upload the JSON string\n",
    "    s3.put_object(\n",
    "    Bucket=bucket_name,\n",
    "    Key=s3_object_key,\n",
    "    Body=ticker['json_data']\n",
    ")\n",
    "  \n",
    "\n",
    "    print(f\"Successfully uploaded JSON data to s3://{bucket_name}/{s3_object_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d67b5a1-0792-43c9-a52d-00cf16efb576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The Verification: Standards Check\n",
    "\n",
    "\n",
    "Resilience (Fail Safely): PASS. You use try/except and route errors to audit_failures instead of crashing. \n",
    "\n",
    "\n",
    "\n",
    "Observability: PASS. You capture the exact error message and status code in the DLQ JSON. \n",
    "\n",
    "\n",
    "Disaster Recovery: PASS. You correctly added the widget run_date, allowing you to run \"Protocol A: The Surgical Backfill\"  by passing a specific date.\n",
    "\n",
    "Data Integrity: PASS. You removed the second json.dumps(), preventing the \"Double String\" corruption. The data will arrive in Bronze as a proper Struct."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Landing Zone",
   "widgets": {
    "run_date": {
     "currentValue": "2025-12-25",
     "nuid": "e281c13c-d396-458e-b4c7-21f6e819d272",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2025-12-25",
      "label": "Run date",
      "name": "run_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2025-12-25",
      "label": "Run date",
      "name": "run_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
